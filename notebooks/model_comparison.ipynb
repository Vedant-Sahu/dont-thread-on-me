{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Comparison on Test Set\n",
    "\n",
    "Loads trained models and evaluates on **test set** (2022-10 to 2023-09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    GRAPH_DIR = Path('/content/drive/MyDrive/CS224W/graphs')\n",
    "    PROJECT_DIR = Path('/content/drive/MyDrive/CS224W')\n",
    "else:\n",
    "    GRAPH_DIR = Path('../data/processed/graphs')\n",
    "    PROJECT_DIR = Path('..')\n",
    "\n",
    "RESULTS_DIR = PROJECT_DIR / 'results'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (Linear + RF): ../results/baseline/baseline_models.pkl ✓\n",
      "Temporal GNN: ../results/temporal_gnn.pt ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION - Add your models here\n",
    "# ============================================================================\n",
    "MODELS = {\n",
    "    \"Baseline (Linear + RF)\": \"../results/baseline/baseline_models.pkl\",\n",
    "    \"Temporal GNN\": \"../results/temporal_gnn.pt\",\n",
    "    # \"Baseline GNN\": \"../results/baseline_gnn.pt\",\n",
    "}\n",
    "\n",
    "for name, path in MODELS.items():\n",
    "    exists = Path(path).exists()\n",
    "    print(f\"{name}: {path} {'✓' if exists else '✗ NOT FOUND'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gnn-class-header",
   "metadata": {},
   "source": [
    "## 1. GNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gnn-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalCommunityGNN(nn.Module):\n",
    "    def __init__(self, user_feat_dim: int, tag_feat_dim: int, hidden_dim: int = 128,\n",
    "                 num_conv_layers: int = 2, num_transformer_layers: int = 2,\n",
    "                 num_attention_heads: int = 4, dropout: float = 0.1,\n",
    "                 transformer_ffn_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.user_norm = nn.LayerNorm(user_feat_dim)\n",
    "        self.tag_norm = nn.LayerNorm(tag_feat_dim)\n",
    "        self.user_proj = nn.Linear(user_feat_dim, hidden_dim)\n",
    "        self.tag_proj = nn.Linear(tag_feat_dim, hidden_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_conv_layers):\n",
    "            conv = HeteroConv({\n",
    "                (\"tag\", \"cooccurs\", \"tag\"): SAGEConv(hidden_dim, hidden_dim, aggr=\"mean\"),\n",
    "                (\"user\", \"contributes\", \"tag\"): SAGEConv((hidden_dim, hidden_dim), hidden_dim, aggr=\"mean\"),\n",
    "                (\"tag\", \"contributed_to_by\", \"user\"): SAGEConv((hidden_dim, hidden_dim), hidden_dim, aggr=\"mean\"),\n",
    "            }, aggr=\"mean\")\n",
    "            self.convs.append(conv)\n",
    "        \n",
    "        self.conv_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        community_emb_dim = 2 * hidden_dim\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=community_emb_dim, nhead=num_attention_heads,\n",
    "            dim_feedforward=transformer_ffn_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "        \n",
    "        self.qpd_head = nn.Linear(community_emb_dim, 1)\n",
    "        self.ansrate_head = nn.Linear(community_emb_dim, 1)\n",
    "        self.retention_head = nn.Linear(community_emb_dim, 1)\n",
    "\n",
    "    def forward_single_graph(self, x_dict, edge_index_dict):\n",
    "        projected = {}\n",
    "        if \"user\" in x_dict:\n",
    "            projected[\"user\"] = self.user_proj(self.user_norm(x_dict[\"user\"]))\n",
    "        if \"tag\" in x_dict:\n",
    "            projected[\"tag\"] = self.tag_proj(self.tag_norm(x_dict[\"tag\"]))\n",
    "        \n",
    "        x = projected\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index_dict)\n",
    "            x = {k: torch.relu(v) for k, v in x.items()}\n",
    "            x = {k: self.conv_dropout(v) for k, v in x.items()}\n",
    "        \n",
    "        user_pooled = x[\"user\"].mean(dim=0)\n",
    "        tag_pooled = x[\"tag\"].mean(dim=0)\n",
    "        return torch.cat([user_pooled, tag_pooled])\n",
    "\n",
    "    def forward(self, batch_monthly_graphs):\n",
    "        batch_embeddings = []\n",
    "        for community_graphs in batch_monthly_graphs:\n",
    "            monthly_embs = []\n",
    "            for graph in community_graphs:\n",
    "                if isinstance(graph, tuple):\n",
    "                    x_dict, edge_index_dict = graph\n",
    "                else:\n",
    "                    x_dict = graph.x_dict\n",
    "                    edge_index_dict = graph.edge_index_dict\n",
    "                emb = self.forward_single_graph(x_dict, edge_index_dict)\n",
    "                monthly_embs.append(emb)\n",
    "            batch_embeddings.append(torch.stack(monthly_embs))\n",
    "        \n",
    "        batch_embeddings = torch.stack(batch_embeddings)\n",
    "        temporal_out = self.temporal_encoder(batch_embeddings)\n",
    "        final_repr = temporal_out[:, -1, :]\n",
    "        \n",
    "        return {\n",
    "            \"qpd\": self.qpd_head(final_repr).squeeze(-1),\n",
    "            \"answer_rate\": self.ansrate_head(final_repr).squeeze(-1),\n",
    "            \"retention\": self.retention_head(final_repr).squeeze(-1),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 2. Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: 2002 samples\n"
     ]
    }
   ],
   "source": [
    "class TestDataset:\n",
    "    SPLIT_RANGES = {\n",
    "        'train': ('2014-01', '2020-06'),\n",
    "        'val':   ('2020-07', '2022-09'),\n",
    "        'test':  ('2022-10', '2023-09')\n",
    "    }\n",
    "    \n",
    "    def __init__(self, graph_dir: Path, split: str = 'test', seq_len: int = 12, horizon: int = 6):\n",
    "        self.graph_dir = Path(graph_dir)\n",
    "        self.split = split\n",
    "        self.seq_len = seq_len\n",
    "        self.horizon = horizon\n",
    "        self.cache = {}\n",
    "        self.samples = self._build_index()\n",
    "        print(f\"{split.upper()}: {len(self.samples)} samples\")\n",
    "    \n",
    "    def _load(self, community, month):\n",
    "        key = (community, month)\n",
    "        if key not in self.cache:\n",
    "            self.cache[key] = torch.load(self.graph_dir / community / f\"{month}.pt\", \n",
    "                                         weights_only=False, map_location='cpu')\n",
    "        return self.cache[key]\n",
    "    \n",
    "    def _build_index(self):\n",
    "        samples = []\n",
    "        start, end = self.SPLIT_RANGES[self.split]\n",
    "        \n",
    "        for comm_dir in sorted(self.graph_dir.iterdir()):\n",
    "            if not comm_dir.is_dir(): continue\n",
    "            months = sorted([f.stem for f in comm_dir.glob('*.pt')])\n",
    "            if len(months) < self.seq_len + self.horizon: continue\n",
    "            \n",
    "            for i, m in enumerate(months):\n",
    "                if not (start <= m <= end): continue\n",
    "                if i < self.seq_len - 1: continue\n",
    "                target_idx = i + self.horizon\n",
    "                if target_idx >= len(months): continue\n",
    "                \n",
    "                seq = months[i - self.seq_len + 1:i + 1]\n",
    "                target = months[target_idx]\n",
    "                \n",
    "                if self._consecutive(seq, target):\n",
    "                    samples.append({'comm': comm_dir.name, 'seq': seq, 'target': target})\n",
    "        return samples\n",
    "    \n",
    "    def _consecutive(self, seq, target):\n",
    "        try:\n",
    "            dates = [datetime.strptime(m, '%Y-%m') for m in seq]\n",
    "            for i in range(1, len(dates)):\n",
    "                if dates[i] != dates[i-1] + relativedelta(months=1): return False\n",
    "            t = datetime.strptime(target, '%Y-%m')\n",
    "            return t == dates[-1] + relativedelta(months=self.horizon)\n",
    "        except: return False\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        graphs = [self._load(s['comm'], m) for m in s['seq']]\n",
    "        target = self._load(s['comm'], s['target'])\n",
    "        return graphs, target.y, graphs[-1].y\n",
    "\n",
    "test_dataset = TestDataset(GRAPH_DIR, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features-header",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction (for baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_graph_features(g):\n",
    "    feats = []\n",
    "    user_x = g['user'].x.numpy()\n",
    "    if len(user_x) > 0:\n",
    "        feats.extend(user_x.mean(0)); feats.extend(user_x.std(0)); feats.append(len(user_x))\n",
    "    else:\n",
    "        feats.extend([0]*11)\n",
    "    \n",
    "    tag_x = g['tag'].x.numpy()\n",
    "    if len(tag_x) > 0:\n",
    "        feats.extend(tag_x.mean(0)); feats.extend(tag_x.std(0)); feats.append(len(tag_x))\n",
    "    else:\n",
    "        feats.extend([0]*15)\n",
    "    \n",
    "    for et in [('user', 'posts_in', 'tag'), ('user', 'answers', 'user')]:\n",
    "        feats.append(g[et].edge_index.shape[1] if et in g.edge_types else 0)\n",
    "    \n",
    "    for k in ['qpd', 'answer_rate', 'retention']:\n",
    "        feats.append(float(g.y.get(k, 0)) if hasattr(g, 'y') else 0)\n",
    "    \n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def extract_seq_features(graphs):\n",
    "    gf = np.array([extract_graph_features(g) for g in graphs])\n",
    "    feats = list(gf[-1]) + list(gf.mean(0)) + list(gf.std(0))\n",
    "    x = np.arange(len(graphs))\n",
    "    for i in range(gf.shape[1]):\n",
    "        feats.append(np.polyfit(x, gf[:, i], 1)[0] if np.std(gf[:, i]) > 1e-8 else 0)\n",
    "    return np.array(feats, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extract-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f254cdd0394c28b80d1fc2405b2484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/2002 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 2002 samples, 124 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting test features...\")\n",
    "X_test, y_test, y_curr = [], {'qpd': [], 'answer_rate': [], 'retention': []}, {'qpd': [], 'answer_rate': [], 'retention': []}\n",
    "test_graphs_for_gnn = []\n",
    "\n",
    "for i in tqdm(range(len(test_dataset)), desc='Test'):\n",
    "    graphs, targets, current = test_dataset[i]\n",
    "    X_test.append(extract_seq_features(graphs))\n",
    "    test_graphs_for_gnn.append(graphs)\n",
    "    for k in y_test:\n",
    "        y_test[k].append(float(targets.get(k, 0)))\n",
    "        y_curr[k].append(float(current.get(k, 0)))\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "for k in y_test:\n",
    "    y_test[k] = np.array(y_test[k])\n",
    "    y_curr[k] = np.array(y_curr[k])\n",
    "\n",
    "print(f\"Test: {X_test.shape[0]} samples, {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 4. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eval-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = ['qpd', 'answer_rate', 'retention']\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    return {\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'r2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def print_results(name, results):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for t in TARGETS:\n",
    "        r = results[t]\n",
    "        print(f\"{t:15} MAE={r['mae']:.4f}  RMSE={r['rmse']:.4f}  R²={r['r2']:.4f}\")\n",
    "    mean_r2 = np.mean([results[t]['r2'] for t in TARGETS])\n",
    "    print(f\"{'MEAN R²':15} {mean_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 5. Load and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Naive (predict current)\n",
      "============================================================\n",
      "qpd             MAE=1.0394  RMSE=2.3570  R²=0.9859\n",
      "answer_rate     MAE=0.1104  RMSE=0.1658  R²=-0.0680\n",
      "retention       MAE=0.0903  RMSE=0.1407  R²=-0.0091\n",
      "MEAN R²         0.3029\n",
      "\n",
      ">>> Loading Baseline (Linear + RF) from ../results/baseline/baseline_models.pkl\n",
      "\n",
      "============================================================\n",
      "Linear Regression\n",
      "============================================================\n",
      "qpd             MAE=1.6566  RMSE=2.6704  R²=0.9819\n",
      "answer_rate     MAE=0.1087  RMSE=0.1547  R²=0.0702\n",
      "retention       MAE=0.0797  RMSE=0.1226  R²=0.2336\n",
      "MEAN R²         0.4286\n",
      "\n",
      "============================================================\n",
      "Random Forest\n",
      "============================================================\n",
      "qpd             MAE=1.6494  RMSE=9.5714  R²=0.7671\n",
      "answer_rate     MAE=0.1092  RMSE=0.1508  R²=0.1168\n",
      "retention       MAE=0.0777  RMSE=0.1215  R²=0.2482\n",
      "MEAN R²         0.3774\n",
      "\n",
      ">>> Loading Temporal GNN from ../results/temporal_gnn.pt\n",
      "  Arch: hidden=128, conv=3, transformer=3, ffn=256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1542dab37aa40abb3dfdf6825452f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Temporal GNN:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "# Naive baseline\n",
    "naive_results = {t: evaluate(y_test[t], y_curr[t]) for t in TARGETS}\n",
    "all_results['Naive'] = naive_results\n",
    "print_results('Naive (predict current)', naive_results)\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    model_path = Path(model_path)\n",
    "    if not model_path.exists():\n",
    "        print(f\"\\n⚠️  {model_name}: NOT FOUND at {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n>>> Loading {model_name} from {model_path}\")\n",
    "    \n",
    "    # ===== BASELINE MODELS (.pkl) =====\n",
    "    if model_path.suffix == '.pkl':\n",
    "        with open(model_path, 'rb') as f:\n",
    "            saved = pickle.load(f)\n",
    "        \n",
    "        scaler = saved.get('scaler')\n",
    "        norm_stats = saved.get('norm_stats', {})\n",
    "        X_scaled = scaler.transform(X_test) if scaler else X_test\n",
    "        \n",
    "        # Linear Regression\n",
    "        if 'linear_models' in saved:\n",
    "            results = {}\n",
    "            for t in TARGETS:\n",
    "                if t in saved['linear_models']:\n",
    "                    pred = saved['linear_models'][t].predict(X_scaled)\n",
    "                    if t in norm_stats:\n",
    "                        pred = pred * norm_stats[t]['std'] + norm_stats[t]['mean']\n",
    "                    results[t] = evaluate(y_test[t], pred)\n",
    "            if results:\n",
    "                all_results['Linear Regression'] = results\n",
    "                print_results('Linear Regression', results)\n",
    "        \n",
    "        # Random Forest\n",
    "        if 'rf_models' in saved:\n",
    "            results = {}\n",
    "            for t in TARGETS:\n",
    "                if t in saved['rf_models']:\n",
    "                    pred = saved['rf_models'][t].predict(X_scaled)\n",
    "                    if t in norm_stats:\n",
    "                        pred = pred * norm_stats[t]['std'] + norm_stats[t]['mean']\n",
    "                    results[t] = evaluate(y_test[t], pred)\n",
    "            if results:\n",
    "                all_results['Random Forest'] = results\n",
    "                print_results('Random Forest', results)\n",
    "    \n",
    "    # ===== GNN MODELS (.pt) =====\n",
    "    elif model_path.suffix == '.pt':\n",
    "        checkpoint = torch.load(model_path, weights_only=False, map_location=device)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Infer architecture from state dict\n",
    "        hidden_dim = state_dict['user_proj.weight'].shape[0]\n",
    "        num_conv_layers = max(int(k.split('.')[1]) for k in state_dict if k.startswith('convs.')) + 1\n",
    "        num_transformer_layers = max(int(k.split('.')[2]) for k in state_dict if 'temporal_encoder.layers' in k) + 1\n",
    "        transformer_ffn_dim = state_dict['temporal_encoder.layers.0.linear1.weight'].shape[0]\n",
    "        \n",
    "        print(f\"  Arch: hidden={hidden_dim}, conv={num_conv_layers}, transformer={num_transformer_layers}, ffn={transformer_ffn_dim}\")\n",
    "        \n",
    "        model = TemporalCommunityGNN(\n",
    "            user_feat_dim=5, tag_feat_dim=7,\n",
    "            hidden_dim=hidden_dim, num_conv_layers=num_conv_layers,\n",
    "            num_transformer_layers=num_transformer_layers, num_attention_heads=4,\n",
    "            dropout=0.1, transformer_ffn_dim=transformer_ffn_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        preds = {t: [] for t in TARGETS}\n",
    "        batch_size = 32\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(test_graphs_for_gnn), batch_size), desc=f'Eval {model_name}'):\n",
    "                batch = test_graphs_for_gnn[i:i+batch_size]\n",
    "                batch = [[g.to(device) for g in seq] for seq in batch]\n",
    "                out = model(batch)\n",
    "                for t in TARGETS:\n",
    "                    preds[t].extend(out[t].cpu().numpy())\n",
    "        \n",
    "        results = {t: evaluate(y_test[t], np.array(preds[t])) for t in TARGETS}\n",
    "        all_results[model_name] = results\n",
    "        print_results(model_name, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "rows = []\n",
    "for model_name, results in all_results.items():\n",
    "    for t in TARGETS:\n",
    "        if t in results:\n",
    "            rows.append({'Model': model_name, 'Target': t, **results[t]})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEAN R² BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "mean_df = df.groupby('Model')['r2'].mean().sort_values(ascending=False)\n",
    "print(mean_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output = {'test_samples': len(test_dataset), 'models': {}}\n",
    "for name, results in all_results.items():\n",
    "    output['models'][name] = {\n",
    "        'metrics': {t: {k: float(v) for k, v in results[t].items()} for t in TARGETS if t in results},\n",
    "        'mean_r2': float(np.mean([results[t]['r2'] for t in TARGETS if t in results]))\n",
    "    }\n",
    "\n",
    "with open(RESULTS_DIR / 'test_set_results.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "print(f\"Saved to {RESULTS_DIR / 'test_set_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39967ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
