{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Baseline GNN Training\n",
        "\n",
        "- Loads model from `baseline_gnn.py` file\n",
        "- Starts training from 2014 (skips noisy early years)\n",
        "- Uses 2K stratified samples for fast iteration\n",
        "- Lazy loading with LRU cache\n",
        "- Works both locally and on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXKW_Lj5hHYR"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mHWpjST0hHYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c68c8bb-8b31-468c-c62d-924551060f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Google Colab\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Check if running on Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running on Google Colab\")\n",
        "    !pip install torch-geometric\n",
        "    !pip install python-dateutil\n",
        "else:\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QSbnXaffhHYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0193c62f-53f7-4a36-a1c3-b92d5522f4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: nvidia a100-sxm4-80gb\n",
            "GPU memory: 85.17 GB\n",
            "Detected Hopper/Ampere GPU → using BF16\n",
            "\n",
            "Final Precision Settings\n",
            "Using device: cuda\n",
            "AMP enabled:  True\n",
            "BF16 enabled: True\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "# Default values\n",
        "USE_BF16 = False\n",
        "USE_AMP = torch.cuda.is_available()  # AMP works only on CUDA\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "    print(f\"GPU available: {gpu_name}\")\n",
        "    print(f\"GPU memory: {gpu_mem:.2f} GB\")\n",
        "\n",
        "    # Precision selection based on GPU family\n",
        "    if any(key in gpu_name for key in ['b200', 'b100', 'blackwell']):\n",
        "        print(\"Detected Blackwell GPU → using BF16 for optimal performance\")\n",
        "        USE_BF16 = True\n",
        "\n",
        "    elif any(key in gpu_name for key in ['h100', 'a100']):\n",
        "        print(\"Detected Hopper/Ampere GPU → using BF16\")\n",
        "        USE_BF16 = True\n",
        "\n",
        "    else:\n",
        "        print(\"Standard CUDA GPU detected → using FP16 (AMP)\")\n",
        "        USE_BF16 = False\n",
        "\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"No GPU available → using CPU\")\n",
        "    USE_BF16 = False\n",
        "    USE_AMP = False\n",
        "\n",
        "print(\"\\nFinal Precision Settings\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"AMP enabled:  {USE_AMP}\")\n",
        "print(f\"BF16 enabled: {USE_BF16}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q0M-vbe1hHYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48f7aba-d587-4e18-c5a4-a4f569d353c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted\n",
            "Project directory: /content/drive/MyDrive/dont-thread-on-me\n",
            "Graph directory: /content/drive/MyDrive/dont-thread-on-me/data/processed/graphs\n"
          ]
        }
      ],
      "source": [
        "# Setup paths based on environment\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    PROJECT_DIR = '/content/drive/MyDrive/dont-thread-on-me'\n",
        "    GRAPH_DIR = f'{PROJECT_DIR}/data/processed/graphs'\n",
        "\n",
        "    # Add project directory to path for imports\n",
        "    if PROJECT_DIR not in sys.path:\n",
        "        sys.path.append(PROJECT_DIR)\n",
        "\n",
        "    print(f\"Google Drive mounted\")\n",
        "    print(f\"Project directory: {PROJECT_DIR}\")\n",
        "\n",
        "else:\n",
        "    PROJECT_DIR = '..'\n",
        "    GRAPH_DIR = '../data/processed/graphs'\n",
        "\n",
        "    if PROJECT_DIR not in sys.path:\n",
        "        sys.path.append(PROJECT_DIR)\n",
        "\n",
        "    print(f\"Running locally\")\n",
        "    print(f\"Project directory: {PROJECT_DIR}\")\n",
        "\n",
        "print(f\"Graph directory: {GRAPH_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "verify_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855e03b4-ca0c-486b-88e2-55a27e33c1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 177 communities\n",
            "  - 3dprinting.stackexchange.com: 98 monthly graphs\n",
            "  - academia.stackexchange.com: 147 monthly graphs\n",
            "  - ai.stackexchange.com: 91 monthly graphs\n",
            "  - android.stackexchange.com: 174 monthly graphs\n",
            "  - anime.stackexchange.com: 135 monthly graphs\n",
            "  ... and 172 more\n"
          ]
        }
      ],
      "source": [
        "# Verify data directory exists\n",
        "from pathlib import Path\n",
        "\n",
        "graph_dir = Path(GRAPH_DIR)\n",
        "\n",
        "if not graph_dir.exists():\n",
        "    print(f\"ERROR: Graph directory not found: {graph_dir}\")\n",
        "    print(f\"\\nPlease upload your processed graphs to:\")\n",
        "    print(f\"  {GRAPH_DIR}\")\n",
        "else:\n",
        "    communities = [d.name for d in graph_dir.iterdir() if d.is_dir()]\n",
        "    print(f\"Found {len(communities)} communities\")\n",
        "\n",
        "    # Show first few\n",
        "    for comm in sorted(communities)[:5]:\n",
        "        comm_path = graph_dir / comm\n",
        "        n_graphs = len(list(comm_path.glob('*.pt')))\n",
        "        print(f\"  - {comm}: {n_graphs} monthly graphs\")\n",
        "\n",
        "    if len(communities) > 5:\n",
        "        print(f\"  ... and {len(communities) - 5} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_model"
      },
      "source": [
        "## 2. Import Model from baseline_gnn.py\n",
        "\n",
        "**Note:** Make sure `baseline_gnn.py` is uploaded to your project directory:\n",
        "- Colab: `/content/drive/MyDrive/dont-thread-on-me/src/models/baseline_gnn.py`\n",
        "- Local: `../src/models/baseline_gnn.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "import_model_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de80c45d-544a-48ce-ab07-1b35b925fb52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully imported model from baseline_gnn.py\n"
          ]
        }
      ],
      "source": [
        "# Try to import the model\n",
        "try:\n",
        "    from src.models.baseline_gnn import create_model\n",
        "    print(\"Successfully imported model from baseline_gnn.py\")\n",
        "except ImportError as e:\n",
        "    print(\"Failed to import model\")\n",
        "    print(f\"\\nError: {e}\")\n",
        "    print(f\"\\nPlease make sure baseline_gnn.py exists at:\")\n",
        "    if IN_COLAB:\n",
        "        print(f\"  /content/drive/MyDrive/dont-thread-on-me/src/models/baseline_gnn.py\")\n",
        "    else:\n",
        "        print(f\"  ../src/models/baseline_gnn.py\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_section"
      },
      "source": [
        "## 3. Define Optimized Dataset\n",
        "\n",
        "- **No pre-caching** - starts training immediately\n",
        "- **LRU cache** - automatically caches frequently accessed graphs\n",
        "- **Stratified sampling** - representative samples across communities\n",
        "- **2014 start date** - skips noisy early Stack Exchange years"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "8rSx-wsZB1B0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "optimized_dataset"
      },
      "outputs": [],
      "source": [
        "class FastTemporalDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Optimized dataset with lazy loading + LRU cache + target normalization.\n",
        "\n",
        "    - No slow pre-caching step (starts training immediately)\n",
        "    - LRU cache automatically keeps hot graphs in memory\n",
        "    - Memory efficient (only caches ~2000 graphs, not all)\n",
        "    - Stratified sampling for representative coverage\n",
        "    - Starts from 2014 (skips noisy early years)\n",
        "    - Target normalization: Standardizes all targets to mean=0, std=1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        graph_dir: Path,\n",
        "        split: str = 'train',\n",
        "        sequence_length: int = 12,\n",
        "        prediction_horizon: int = 6,\n",
        "        max_samples: int = None,\n",
        "        cache_size: int = 2000,\n",
        "        stratified_sample: bool = True\n",
        "    ):\n",
        "        self.graph_dir = Path(graph_dir)\n",
        "        self.split = split\n",
        "        self.sequence_length = sequence_length\n",
        "        self.prediction_horizon = prediction_horizon\n",
        "\n",
        "        # Temporal splits\n",
        "        self.split_ranges = {\n",
        "            'train': ('2014-01', '2020-06'),  # Predict through 2020-12\n",
        "            'val':   ('2020-07', '2022-09'),  # Predict through 2023-03\n",
        "            'test':  ('2022-10', '2023-09')   # Predict through 2024-03\n",
        "        }\n",
        "\n",
        "        # Build sample index\n",
        "        print(f\"Building {split} sample index...\")\n",
        "        self.samples = self._build_sample_index()\n",
        "        print(f\"  Found {len(self.samples)} potential samples\")\n",
        "\n",
        "        # Apply sampling strategy\n",
        "        if max_samples and len(self.samples) > max_samples:\n",
        "            if stratified_sample:\n",
        "                print(f\"  Applying stratified sampling to select {max_samples} samples...\")\n",
        "                self.samples = self._stratified_sample(max_samples)\n",
        "            else:\n",
        "                print(f\"  Randomly sampling {max_samples} samples...\")\n",
        "                random.shuffle(self.samples)\n",
        "                self.samples = self.samples[:max_samples]\n",
        "\n",
        "        print(f\"{split.upper()} Dataset: {len(self.samples)} samples\")\n",
        "\n",
        "        # Initialize normalization stats (will be computed for train, loaded for val/test)\n",
        "        self.norm_stats = None\n",
        "\n",
        "        # Create LRU cached loader for individual monthly graphs\n",
        "        @lru_cache(maxsize=cache_size)\n",
        "        def _cached_load(community: str, month: str):\n",
        "            path = self.graph_dir / community / f\"{month}.pt\"\n",
        "            return torch.load(path, weights_only=False, map_location='cpu')\n",
        "\n",
        "        self._load_graph = _cached_load\n",
        "        self._access_count = 0\n",
        "\n",
        "    def compute_normalization_stats(self):\n",
        "        \"\"\"\n",
        "        Compute mean and std for each target metric from training data.\n",
        "\n",
        "        IMPORTANT: Only call this on the training dataset.\n",
        "        Validation and test datasets will use these same statistics.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"Computing target normalization statistics\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"Sampling up to 1000 examples to compute statistics...\")\n",
        "\n",
        "        all_targets = {\n",
        "            'qpd': [],\n",
        "            'answer_rate': [],\n",
        "            'retention': []\n",
        "        }\n",
        "\n",
        "        # Sample up to 1000 examples for statistics\n",
        "        sample_size = min(1000, len(self.samples))\n",
        "        sample_indices = np.random.choice(len(self.samples), sample_size, replace=False)\n",
        "\n",
        "        for idx in tqdm(sample_indices, desc='Sampling targets'):\n",
        "            sample = self.samples[idx]\n",
        "            target_graph = self._load_graph(sample['community'], sample['target_month'])\n",
        "            targets = target_graph.y\n",
        "\n",
        "            for key in all_targets:\n",
        "                if key in targets:\n",
        "                    all_targets[key].append(float(targets[key]))\n",
        "\n",
        "        # Compute mean and std for each metric\n",
        "        self.norm_stats = {}\n",
        "        for key in all_targets:\n",
        "            if len(all_targets[key]) > 0:\n",
        "                values = np.array(all_targets[key])\n",
        "                self.norm_stats[key] = {\n",
        "                    'mean': float(values.mean()),\n",
        "                    'std': float(values.std() + 1e-8)  # Add small constant to avoid division by zero\n",
        "                }\n",
        "\n",
        "        print(\"\\nNormalization statistics (computed from training data):\")\n",
        "        print(f\"{'Metric':<15} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
        "        print(\"-\" * 63)\n",
        "\n",
        "        for key in all_targets:\n",
        "            if key in self.norm_stats:\n",
        "                values = np.array(all_targets[key])\n",
        "                print(f\"{key:<15} {self.norm_stats[key]['mean']:<12.4f} \"\n",
        "                      f\"{self.norm_stats[key]['std']:<12.4f} \"\n",
        "                      f\"{values.min():<12.4f} {values.max():<12.4f}\")\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"  Normalization statistics computed\")\n",
        "        print(\"  These will be used to standardize all targets during training\")\n",
        "        print(\"  Val/test datasets will use these SAME statistics (no data leakage)\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "        return self.norm_stats\n",
        "\n",
        "    def _build_sample_index(self) -> List[Dict]:\n",
        "        \"\"\"Build index of all valid temporal sequences.\"\"\"\n",
        "        samples = []\n",
        "        start_month, end_month = self.split_ranges[self.split]\n",
        "        min_graphs = self.sequence_length + self.prediction_horizon\n",
        "\n",
        "        for community_dir in sorted(self.graph_dir.iterdir()):\n",
        "            if not community_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            available_months = sorted([f.stem for f in community_dir.glob('*.pt')])\n",
        "            if len(available_months) < min_graphs:\n",
        "                continue\n",
        "\n",
        "            for i, month_t in enumerate(available_months):\n",
        "                # Check if month is in split range\n",
        "                if not (start_month <= month_t <= end_month):\n",
        "                    continue\n",
        "                if i < self.sequence_length - 1:\n",
        "                    continue\n",
        "\n",
        "                target_idx = i + self.prediction_horizon\n",
        "                if target_idx >= len(available_months):\n",
        "                    continue\n",
        "\n",
        "                seq_start = i - self.sequence_length + 1\n",
        "                seq_months = available_months[seq_start:i+1]\n",
        "                target_month = available_months[target_idx]\n",
        "\n",
        "                # Check temporal consistency\n",
        "                if self._is_consecutive(seq_months, target_month):\n",
        "                    samples.append({\n",
        "                        'community': community_dir.name,\n",
        "                        'sequence_months': seq_months,\n",
        "                        'target_month': target_month\n",
        "                    })\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _is_consecutive(self, seq_months, target_month):\n",
        "        \"\"\"Check if months form consecutive sequence.\"\"\"\n",
        "        try:\n",
        "            dates = [datetime.strptime(m, '%Y-%m') for m in seq_months]\n",
        "            for i in range(1, len(dates)):\n",
        "                if dates[i] != dates[i-1] + relativedelta(months=1):\n",
        "                    return False\n",
        "            target_date = datetime.strptime(target_month, '%Y-%m')\n",
        "            expected = dates[-1] + relativedelta(months=self.prediction_horizon)\n",
        "            return target_date == expected\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def _stratified_sample(self, n_samples: int) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Create stratified sample across communities.\n",
        "        Ensures representative distribution across small/medium/large communities.\n",
        "        \"\"\"\n",
        "        # Group by community\n",
        "        by_community = defaultdict(list)\n",
        "        for sample in self.samples:\n",
        "            by_community[sample['community']].append(sample)\n",
        "\n",
        "        # Sample proportionally from each community\n",
        "        samples_per_community = max(1, n_samples // len(by_community))\n",
        "        selected = []\n",
        "\n",
        "        for community, comm_samples in by_community.items():\n",
        "            n = min(samples_per_community, len(comm_samples))\n",
        "            selected.extend(random.sample(comm_samples, n))\n",
        "\n",
        "        # Trim to exact size and shuffle\n",
        "        random.shuffle(selected)\n",
        "        return selected[:n_samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        community = sample['community']\n",
        "\n",
        "        # Load sequence using cached loader\n",
        "        graphs = [\n",
        "            self._load_graph(community, month)\n",
        "            for month in sample['sequence_months']\n",
        "        ]\n",
        "\n",
        "        target_graph = self._load_graph(community, sample['target_month'])\n",
        "        targets = target_graph.y\n",
        "\n",
        "        # Apply standardization if statistics are available\n",
        "        if self.norm_stats is not None:\n",
        "            targets_standardized = {}\n",
        "            for key in targets:\n",
        "                if key in self.norm_stats:\n",
        "                    # Z-score normalization: (x - mean) / std\n",
        "                    targets_standardized[key] = (\n",
        "                        (targets[key] - self.norm_stats[key]['mean']) /\n",
        "                        self.norm_stats[key]['std']\n",
        "                    )\n",
        "                else:\n",
        "                    targets_standardized[key] = targets[key]\n",
        "            targets = targets_standardized\n",
        "\n",
        "        # Periodic cache statistics (every 1000 accesses)\n",
        "        self._access_count += 1\n",
        "        if self._access_count % 1000 == 0:\n",
        "            cache_info = self._load_graph.cache_info()\n",
        "            if cache_info.hits + cache_info.misses > 0:\n",
        "                hit_rate = cache_info.hits / (cache_info.hits + cache_info.misses)\n",
        "                print(f\"  Cache: {cache_info.currsize}/{cache_info.maxsize} graphs | Hit rate: {hit_rate:.1%}\")\n",
        "\n",
        "        return graphs, targets\n",
        "\n",
        "    def get_cache_stats(self):\n",
        "        \"\"\"Get current cache statistics.\"\"\"\n",
        "        return self._load_graph.cache_info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "collate_fn"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for batching.\"\"\"\n",
        "    batch_graphs = []\n",
        "    batch_targets = {'qpd': [], 'answer_rate': [], 'retention': []}\n",
        "\n",
        "    for graphs, targets in batch:\n",
        "        batch_graphs.append(graphs)\n",
        "        for key in batch_targets:\n",
        "            batch_targets[key].append(targets[key])\n",
        "\n",
        "    for key in batch_targets:\n",
        "        batch_targets[key] = torch.tensor(batch_targets[key], dtype=torch.float32)\n",
        "\n",
        "    return batch_graphs, batch_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_datasets"
      },
      "source": [
        "## 4. Load Saved Dataset Configuration\n",
        "\n",
        "**Configuration:**\n",
        "- 2,000 training samples (stratified across communities)\n",
        "- 500 validation samples\n",
        "- Starts from 2014 (skips 2008-2013 noisy data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dataset_imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918590c3-6b91-4e84-e3fb-4848b6bcc9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Loaded Temporal GNN Training Configuration\n",
            "======================================================================\n",
            "Training samples:   1834\n",
            "Validation samples: 338\n",
            "Normalization stats: ['qpd', 'answer_rate', 'retention']\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Dataset imports\n",
        "import pickle\n",
        "\n",
        "config_path = f\"{PROJECT_DIR}/results/baseline_config.pkl\"\n",
        "with open(config_path, 'rb') as f:\n",
        "    config = pickle.load(f)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Loaded Temporal GNN Training Configuration\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training samples:   {len(config['train_samples'])}\")\n",
        "print(f\"Validation samples: {len(config['val_samples'])}\")\n",
        "print(f\"Normalization stats: {list(config['norm_stats'].keys())}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "create_datasets_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ab76d-b358-4670-abad-cd5e837fa08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building train sample index...\n",
            "  Found 11020 potential samples\n",
            "TRAIN Dataset: 11020 samples\n",
            "Building val sample index...\n",
            "  Found 4471 potential samples\n",
            "VAL Dataset: 4471 samples\n",
            "\n",
            "Datasets configured with identical samples and normalization\n",
            "Train dataset size: 1834\n",
            "Val dataset size: 338\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = FastTemporalDataset(\n",
        "    graph_dir=GRAPH_DIR,\n",
        "    split='train',\n",
        "    max_samples=None,\n",
        "    stratified_sample=False\n",
        ")\n",
        "\n",
        "val_dataset = FastTemporalDataset(\n",
        "    graph_dir=GRAPH_DIR,\n",
        "    split='val',\n",
        "    max_samples=None,\n",
        "    stratified_sample=False\n",
        ")\n",
        "\n",
        "# Use same data\n",
        "train_dataset.samples = config['train_samples']\n",
        "val_dataset.samples = config['val_samples']\n",
        "train_dataset.norm_stats = config['norm_stats']\n",
        "val_dataset.norm_stats = config['norm_stats']\n",
        "\n",
        "print(\"\\nDatasets configured with identical samples and normalization\")\n",
        "print(f\"Train dataset size: {len(train_dataset.samples)}\")\n",
        "print(f\"Val dataset size: {len(val_dataset.samples)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "inspect_sample",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2bb245-9a96-4bd3-f4c2-68c6bce89be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inspecting first sample...\n",
            "  Sequence length: 12 monthly graphs\n",
            "\n",
            "  Graph structure:\n",
            "    Users: torch.Size([228, 5])\n",
            "    Tags: torch.Size([283, 7])\n",
            "\n",
            "  Target metrics:\n",
            "    qpd: -0.2391\n",
            "    answer_rate: -1.8552\n",
            "    retention: -1.6198\n",
            "    growth: -0.3421\n"
          ]
        }
      ],
      "source": [
        "# Inspect a sample to verify data structure\n",
        "if len(train_dataset) > 0:\n",
        "    print(\"\\nInspecting first sample...\")\n",
        "    sample_graphs, sample_targets = train_dataset[0]\n",
        "    print(f\"  Sequence length: {len(sample_graphs)} monthly graphs\")\n",
        "\n",
        "    g = sample_graphs[0]\n",
        "    print(f\"\\n  Graph structure:\")\n",
        "    print(f\"    Users: {g['user'].x.shape}\")\n",
        "    print(f\"    Tags: {g['tag'].x.shape}\")\n",
        "\n",
        "    print(f\"\\n  Target metrics:\")\n",
        "    for key, value in sample_targets.items():\n",
        "        print(f\"    {key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "create_dataloaders",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bdb9fc9-68f7-4f2b-9f28-4ff87c01b366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  DataLoaders created\n",
            "  Batch size: 64\n",
            "  Train batches: 29\n",
            "  Val batches: 6\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoaders with optimized settings\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=1,  # Parallel loading\n",
        "    pin_memory=True,  # Faster GPU transfer\n",
        "    persistent_workers=True  # Keep workers alive\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(f\"  DataLoaders created\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model_section"
      },
      "source": [
        "## 5. Create Model\n",
        "\n",
        "Using the model imported from `baseline_gnn.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "get_feature_dims",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f03fb56-109b-4c58-d14f-e89b65e5e874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature dimensions:\n",
            "  User features: 5\n",
            "  Tag features: 7\n"
          ]
        }
      ],
      "source": [
        "# Get feature dimensions from data\n",
        "if len(train_dataset) > 0:\n",
        "    sample_graphs, _ = train_dataset[0]\n",
        "    USER_FEAT_DIM = sample_graphs[0]['user'].x.shape[1]\n",
        "    TAG_FEAT_DIM = sample_graphs[0]['tag'].x.shape[1]\n",
        "else:\n",
        "    USER_FEAT_DIM = 5\n",
        "    TAG_FEAT_DIM = 7\n",
        "\n",
        "print(f\"Feature dimensions:\")\n",
        "print(f\"  User features: {USER_FEAT_DIM}\")\n",
        "print(f\"  Tag features: {TAG_FEAT_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "create_model_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3988b86c-c965-4fd7-b889-73c2fffb0f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model created and moved to cuda\n",
            "\n",
            "Model Architecture:\n",
            "  Hidden dimension: 64\n",
            "  Graph conv layers: 2\n",
            "  Transformer layers: 2\n",
            "  Dropout: 0.3\n",
            "\n",
            "Parameters:\n",
            "  Total: 50,843\n",
            "  Trainable: 50,843\n"
          ]
        }
      ],
      "source": [
        "# Model configuration (optimized for 2K samples)\n",
        "model = create_model(\n",
        "    user_feat_dim=USER_FEAT_DIM,\n",
        "    tag_feat_dim=TAG_FEAT_DIM,\n",
        "    hidden_dim=64,           # Reduced for initial training\n",
        "    num_conv_layers=2,       # Reduced to prevent overfitting\n",
        "    dropout=0.3,             # Higher for regularization with limited data\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel created and moved to {device}\")\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  Hidden dimension: 64\")\n",
        "print(f\"  Graph conv layers: 2\")\n",
        "print(f\"  Transformer layers: 2\")\n",
        "print(f\"  Dropout: 0.3\")\n",
        "print(f\"\\nParameters:\")\n",
        "print(f\"  Total: {n_params:,}\")\n",
        "print(f\"  Trainable: {n_trainable:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## 6. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "training_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0872fbb0-e0c3-47a2-f866-03c8749dad21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training functions defined\n"
          ]
        }
      ],
      "source": [
        "def move_to_device(batch_graphs, device):\n",
        "    \"\"\"Move graphs to device efficiently.\"\"\"\n",
        "    return [[g.to(device, non_blocking=True) for g in graphs] for graphs in batch_graphs]\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device, scaler, use_amp, scheduler=None, max_grad_norm=1.0):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    amp_dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "\n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for batch_graphs, batch_targets in pbar:\n",
        "        batch_graphs = move_to_device(batch_graphs, device)\n",
        "        batch_targets = {k: v.to(device, non_blocking=True) for k, v in batch_targets.items()}\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=use_amp, dtype=amp_dtype):\n",
        "            predictions = model(batch_graphs)\n",
        "\n",
        "            # Simple loss (targets are already standardized!)\n",
        "            loss = (\n",
        "                criterion(predictions['qpd'], batch_targets['qpd']) +\n",
        "                criterion(predictions['answer_rate'], batch_targets['answer_rate']) +\n",
        "                criterion(predictions['retention'], batch_targets['retention'])\n",
        "            ) / 3.0\n",
        "\n",
        "        if use_amp and not USE_BF16:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    return total_loss / max(n_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device, use_amp, norm_stats):\n",
        "    \"\"\"\n",
        "    Evaluate model with INVERSE TRANSFORMATION.\n",
        "\n",
        "    IMPORTANT: Metrics are computed on ORIGINAL scale (after denormalization).\n",
        "    This is standard practice - allows interpretation and comparison to baselines.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0  # Loss on standardized scale\n",
        "    n_batches = 0\n",
        "\n",
        "    all_preds = {'qpd': [], 'answer_rate': [], 'retention': []}\n",
        "    all_targets = {'qpd': [], 'answer_rate': [], 'retention': []}\n",
        "\n",
        "    amp_dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "\n",
        "    for batch_graphs, batch_targets in tqdm(loader, desc='Evaluating', leave=False):\n",
        "        batch_graphs = move_to_device(batch_graphs, device)\n",
        "        batch_targets = {k: v.to(device, non_blocking=True) for k, v in batch_targets.items()}\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n",
        "            predictions = model(batch_graphs)\n",
        "\n",
        "            # Compute loss on standardized scale\n",
        "            loss = (\n",
        "                criterion(predictions['qpd'], batch_targets['qpd']) +\n",
        "                criterion(predictions['answer_rate'], batch_targets['answer_rate']) +\n",
        "                criterion(predictions['retention'], batch_targets['retention'])\n",
        "            ) / 3.0\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "        # INVERSE TRANSFORM back to original scale for metrics\n",
        "        for key in predictions:\n",
        "            if key in norm_stats:\n",
        "                # Inverse z-score: x = z * std + mean\n",
        "                preds_original = (\n",
        "                    predictions[key].float().cpu().numpy() * norm_stats[key]['std'] +\n",
        "                    norm_stats[key]['mean']\n",
        "                )\n",
        "                targets_original = (\n",
        "                    batch_targets[key].float().cpu().numpy() * norm_stats[key]['std'] +\n",
        "                    norm_stats[key]['mean']\n",
        "                )\n",
        "            else:\n",
        "                preds_original = predictions[key].float().cpu().numpy()\n",
        "                targets_original = batch_targets[key].float().cpu().numpy()\n",
        "\n",
        "            all_preds[key].extend(preds_original)\n",
        "            all_targets[key].extend(targets_original)\n",
        "\n",
        "    # Compute metrics on ORIGINAL scale (after inverse transform)\n",
        "    metrics = {}\n",
        "    for key in all_preds:\n",
        "        preds = np.array(all_preds[key])\n",
        "        targets = np.array(all_targets[key])\n",
        "\n",
        "        # MAE, R², RMSE on original scale\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        ss_res = np.sum((targets - preds) ** 2)\n",
        "        ss_tot = np.sum((targets - targets.mean()) ** 2)\n",
        "        r2 = 1 - ss_res / (ss_tot + 1e-8)\n",
        "        rmse = np.sqrt(np.mean((targets - preds) ** 2))\n",
        "\n",
        "        metrics[key] = {'mae': mae, 'r2': r2, 'rmse': rmse}\n",
        "\n",
        "    return total_loss / max(n_batches, 1), metrics\n",
        "\n",
        "print(\"Training functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 20  # Should be sufficient with 2K samples\n",
        "PATIENCE = 5     # Early stopping patience\n",
        "USE_AMP = torch.cuda.is_available()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n",
        "criterion = nn.MSELoss()\n",
        "scaler = torch.amp.GradScaler(enabled=USE_AMP and not USE_BF16)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"  AMP: {USE_AMP}\")\n",
        "print(f\"  Dtype: {'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'float32'}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Early stopping patience: {PATIENCE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9zGvXbRariI",
        "outputId": "71c78a06-2e46-44f6-e26c-32dcfda0454d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Configuration:\n",
            "  Device: cuda\n",
            "  AMP: True\n",
            "  Dtype: bfloat16\n",
            "  Batch size: 64\n",
            "  Learning rate: 0.001\n",
            "  Epochs: 20\n",
            "  Early stopping patience: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_test_section"
      },
      "source": [
        "## 7. Quick Test (Single Batch)\n",
        "\n",
        "Verify everything works before full training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PERFORMANCE BENCHMARK - A100 GPU\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE BENCHMARK - Testing on A100 GPU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(train_loader) == 0:\n",
        "    print(\"No data in train_loader\")\n",
        "else:\n",
        "    # Get test batch\n",
        "    batch_graphs, batch_targets = next(iter(train_loader))\n",
        "    batch_graphs = move_to_device(batch_graphs, device)\n",
        "    batch_targets = {k: v.to(device, non_blocking=True) for k, v in batch_targets.items()}\n",
        "\n",
        "    amp_dtype = torch.bfloat16 if USE_BF16 else torch.float16\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 1. WARMUP (important for accurate timing!)\n",
        "    # --------------------------------------------------------\n",
        "    print(\"\\n1. Warming up GPU (JIT compilation, caching)...\")\n",
        "    for _ in range(3):\n",
        "        with torch.amp.autocast('cuda', enabled=USE_AMP, dtype=amp_dtype):\n",
        "            preds = model(batch_graphs)\n",
        "            loss = (\n",
        "                criterion(preds['qpd'], batch_targets['qpd']) +\n",
        "                criterion(preds['answer_rate'], batch_targets['answer_rate']) +\n",
        "                criterion(preds['retention'], batch_targets['retention'])\n",
        "            ) / 3.0\n",
        "        loss.backward()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Warmup complete\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 2. FORWARD PASS ONLY (inference speed)\n",
        "    # --------------------------------------------------------\n",
        "    print(\"\\n2. Testing forward pass (inference)...\")\n",
        "    forward_times = []\n",
        "\n",
        "    for i in range(5):\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=USE_AMP, dtype=amp_dtype):\n",
        "            preds = model(batch_graphs)\n",
        "            loss = (\n",
        "                criterion(preds['qpd'], batch_targets['qpd']) +\n",
        "                criterion(preds['answer_rate'], batch_targets['answer_rate']) +\n",
        "                criterion(preds['retention'], batch_targets['retention'])\n",
        "            ) / 3.0\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = time.time() - start\n",
        "        forward_times.append(elapsed)\n",
        "        print(f\"   Run {i+1}: {elapsed:.3f}s\")\n",
        "\n",
        "    avg_forward = np.mean(forward_times)\n",
        "    std_forward = np.std(forward_times)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 3. FORWARD + BACKWARD PASS (training speed)\n",
        "    # --------------------------------------------------------\n",
        "    print(\"\\n3. Testing forward + backward pass (training)...\")\n",
        "    train_times = []\n",
        "\n",
        "    for i in range(5):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=USE_AMP, dtype=amp_dtype):\n",
        "            preds = model(batch_graphs)\n",
        "            loss = (\n",
        "                criterion(preds['qpd'], batch_targets['qpd']) +\n",
        "                criterion(preds['answer_rate'], batch_targets['answer_rate']) +\n",
        "                criterion(preds['retention'], batch_targets['retention'])\n",
        "            ) / 3.0\n",
        "\n",
        "        if USE_AMP:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = time.time() - start\n",
        "        train_times.append(elapsed)\n",
        "        print(f\"   Run {i+1}: {elapsed:.3f}s (loss: {loss.item():.4f})\")\n",
        "\n",
        "    avg_train = np.mean(train_times)\n",
        "    std_train = np.std(train_times)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 4. RESULTS SUMMARY\n",
        "    # --------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\nForward pass only:\")\n",
        "    print(f\"  Average: {avg_forward:.3f}s ± {std_forward:.3f}s\")\n",
        "\n",
        "    print(f\"\\nForward + Backward (training):\")\n",
        "    print(f\"  Average: {avg_train:.3f}s ± {std_train:.3f}s\")\n",
        "\n",
        "    print(f\"\\nBatch details:\")\n",
        "    print(f\"  Batch size: {batch_targets['qpd'].shape[0]}\")\n",
        "    print(f\"  Samples per batch: {batch_targets['qpd'].shape[0]}\")\n",
        "    print(f\"  Using bfloat16: {USE_BF16}\")\n",
        "    print(f\"  Using AMP: {USE_AMP}\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 5. TIME PROJECTIONS\n",
        "    # --------------------------------------------------------\n",
        "    batches_per_epoch = len(train_loader)\n",
        "    epoch_time_sec = avg_train * batches_per_epoch\n",
        "    epoch_time_min = epoch_time_sec / 60\n",
        "    total_time_min = epoch_time_min * NUM_EPOCHS\n",
        "    total_time_hr = total_time_min / 60\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"TIME PROJECTIONS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  Batch size: {batch_targets['qpd'].shape[0]}\")\n",
        "    print(f\"  Batches per epoch: {batches_per_epoch}\")\n",
        "    print(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
        "\n",
        "    print(f\"\\nEstimated times:\")\n",
        "    print(f\"  Per epoch: {epoch_time_min:.1f} minutes ({epoch_time_sec:.0f} seconds)\")\n",
        "    print(f\"  Total ({NUM_EPOCHS} epochs): {total_time_min:.1f} minutes ({total_time_hr:.2f} hours)\")\n",
        "\n",
        "    # Add validation time estimate (roughly 25% of training time)\n",
        "    val_time_min = (total_time_min * 0.25)\n",
        "    total_with_val = total_time_min + val_time_min\n",
        "\n",
        "    print(f\"  + Validation time: ~{val_time_min:.1f} minutes\")\n",
        "    print(f\"  = Total training time: ~{total_with_val:.1f} minutes ({total_with_val/60:.2f} hours)\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 6. PERFORMANCE ASSESSMENT\n",
        "    # --------------------------------------------------------\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"ASSESSMENT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"  Expected total time: {total_with_val:.0f} minutes ({total_with_val/60:.1f} hours)\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # 7. GPU MEMORY USAGE\n",
        "    # --------------------------------------------------------\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"GPU MEMORY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "        print(f\"\\nCurrent usage:\")\n",
        "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
        "        print(f\"  Peak: {max_allocated:.2f} GB\")\n",
        "\n",
        "        # A100 has 40GB or 80GB\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        if '80GB' in gpu_name:\n",
        "            total_memory = 80\n",
        "        else:\n",
        "            total_memory = 40\n",
        "\n",
        "        usage_pct = (max_allocated / total_memory) * 100\n",
        "        print(f\"  Total GPU memory: {total_memory} GB\")\n",
        "        print(f\"  Usage: {usage_pct:.1f}%\")\n",
        "\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"Benchmark complete! Ready to start training.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Clean up\n",
        "    optimizer.zero_grad()\n",
        "    del batch_graphs, batch_targets, preds, loss\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9EenfpvYFPq",
        "outputId": "ac634630-9832-4af9-b4a6-d0eb90131d0b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PERFORMANCE BENCHMARK - Testing on A100 GPU\n",
            "======================================================================\n",
            "\n",
            "1. Warming up GPU (JIT compilation, caching)...\n",
            "Warmup complete\n",
            "\n",
            "2. Testing forward pass (inference)...\n",
            "   Run 1: 2.660s\n",
            "   Run 2: 2.859s\n",
            "   Run 3: 2.725s\n",
            "   Run 4: 2.734s\n",
            "   Run 5: 2.922s\n",
            "\n",
            "3. Testing forward + backward pass (training)...\n",
            "   Run 1: 5.025s (loss: 1.1614)\n",
            "   Run 2: 4.898s (loss: 1.1623)\n",
            "   Run 3: 4.936s (loss: 1.1623)\n",
            "   Run 4: 4.906s (loss: 1.1617)\n",
            "   Run 5: 5.208s (loss: 1.1623)\n",
            "\n",
            "======================================================================\n",
            "RESULTS\n",
            "======================================================================\n",
            "\n",
            "Forward pass only:\n",
            "  Average: 2.780s ± 0.096s\n",
            "\n",
            "Forward + Backward (training):\n",
            "  Average: 4.995s ± 0.116s\n",
            "\n",
            "Batch details:\n",
            "  Batch size: 64\n",
            "  Samples per batch: 64\n",
            "  Using bfloat16: True\n",
            "  Using AMP: True\n",
            "\n",
            "======================================================================\n",
            "TIME PROJECTIONS\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            "  Training samples: 1834\n",
            "  Batch size: 64\n",
            "  Batches per epoch: 29\n",
            "  Number of epochs: 20\n",
            "\n",
            "Estimated times:\n",
            "  Per epoch: 2.4 minutes (145 seconds)\n",
            "  Total (20 epochs): 48.3 minutes (0.80 hours)\n",
            "  + Validation time: ~12.1 minutes\n",
            "  = Total training time: ~60.4 minutes (1.01 hours)\n",
            "\n",
            "======================================================================\n",
            "ASSESSMENT\n",
            "======================================================================\n",
            "  Expected total time: 60 minutes (1.0 hours)\n",
            "\n",
            "======================================================================\n",
            "GPU MEMORY\n",
            "======================================================================\n",
            "\n",
            "Current usage:\n",
            "  Allocated: 0.12 GB\n",
            "  Reserved: 1.30 GB\n",
            "  Peak: 1.29 GB\n",
            "  Total GPU memory: 80 GB\n",
            "  Usage: 1.6%\n",
            "\n",
            "======================================================================\n",
            "Benchmark complete! Ready to start training.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_loop_section"
      },
      "source": [
        "## 8. Training Loop\n",
        "\n",
        "**Expected training time:** ~2 hours for 20 epochs with 2K samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "30a5632b244648dc92f840ae50ec2dc0",
            "0e89f84209f64c5ea95431c72de5b4cb",
            "f1bcb0c7458e4b8e8e2bbf0f57209702",
            "df73512824764d378af7492f16cdf44a",
            "e34254c196ec4f0d80245cbd49fd2796",
            "08cde1ed291542068785d578c853f8ef",
            "5655cf9c43134764b249af108abfbdb6",
            "8487f56ac643432fa267a25d7617d469",
            "c02e11e183b849e682f284ae988517a1",
            "e23a4692d1fe46ec8c39c7c73e8f5172",
            "994a846c44d94cf9b52474aea4e96811"
          ]
        },
        "outputId": "e0db0698-29c3-49fb-92bf-751a689cf846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Starting Training\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/20\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/29 [12:19<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30a5632b244648dc92f840ae50ec2dc0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Training loop with early stopping - TRACKS ALL METRICS\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Track all metrics for each target variable\n",
        "val_metrics_history = {\n",
        "    'qpd': {'mae': [], 'r2': [], 'rmse': []},\n",
        "    'answer_rate': {'mae': [], 'r2': [], 'rmse': []},\n",
        "    'retention': {'mae': [], 'r2': [], 'rmse': []}\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(\n",
        "        model, train_loader, optimizer, criterion,\n",
        "        device, scaler, USE_AMP, scheduler\n",
        "    )\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Use the training dataset's normalization stats for evaluation\n",
        "    norm_stats = train_dataset.norm_stats\n",
        "\n",
        "    # Validate - returns metrics dict now\n",
        "    val_loss, metrics = evaluate(\n",
        "        model, val_loader, criterion, device, USE_AMP, norm_stats\n",
        "    )\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Store all metrics\n",
        "    for key in metrics:\n",
        "        for metric_name in ['mae', 'r2', 'rmse']:\n",
        "            val_metrics_history[key][metric_name].append(metrics[key][metric_name])\n",
        "\n",
        "    # Learning rate step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    print(f\"\\n  Validation Metrics:\")\n",
        "    print(f\"    {'Metric':<15} {'MAE':<10} {'R²':<10} {'RMSE':<10}\")\n",
        "    print(f\"    {'-'*45}\")\n",
        "    for key in ['qpd', 'answer_rate', 'retention']:\n",
        "        if key in metrics:\n",
        "            print(f\"    {key.replace('_', ' ').title():<15} \"\n",
        "                  f\"{metrics[key]['mae']:<10.3f} \"\n",
        "                  f\"{metrics[key]['r2']:<10.3f} \"\n",
        "                  f\"{metrics[key]['rmse']:<10.3f}\")\n",
        "\n",
        "    # Compute mean metrics across all targets\n",
        "    mean_mae = np.mean([metrics[k]['mae'] for k in metrics])\n",
        "    mean_r2 = np.mean([metrics[k]['r2'] for k in metrics])\n",
        "    mean_rmse = np.mean([metrics[k]['rmse'] for k in metrics])\n",
        "    print(f\"    {'Mean':<15} {mean_mae:<10.3f} {mean_r2:<10.3f} {mean_rmse:<10.3f}\")\n",
        "\n",
        "    # Early stopping check (based on validation loss)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Save best model\n",
        "        if IN_COLAB:\n",
        "            model_path = f\"{PROJECT_DIR}/results/baseline_gnn/best_model.pt\"\n",
        "        else:\n",
        "            model_path = \"../results/baseline_gnn/best_model.pt\"\n",
        "\n",
        "        # Create results directory if it doesn't exist\n",
        "        import os\n",
        "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'metrics': metrics,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'val_metrics_history': val_metrics_history\n",
        "        }, model_path)\n",
        "\n",
        "        print(f\"    New best model saved (val_loss: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    # Show cache statistics every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        cache_info = train_dataset.get_cache_stats()\n",
        "        if cache_info.hits + cache_info.misses > 0:\n",
        "            hit_rate = cache_info.hits / (cache_info.hits + cache_info.misses)\n",
        "            print(f\"\\n  Dataset Cache: {cache_info.currsize}/{cache_info.maxsize} | Hit rate: {hit_rate:.1%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zq8ocYc4ObW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## 9. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"paper\", font_scale=1.2)\n",
        "plt.rcParams['figure.dpi'] = 500\n",
        "plt.rcParams['savefig.dpi'] = 500\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "# Create results directory\n",
        "results_dir = f\"{PROJECT_DIR}/results/baseline_gnn\" if IN_COLAB else \"../results/baseline_gnn\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Color palette\n",
        "colors = sns.color_palette(\"husl\", 3)\n",
        "metric_colors = {'qpd': '#2E86AB', 'answer_rate': '#A23B72', 'retention': '#F18F01'}\n",
        "\n",
        "# ============================================================\n",
        "# Figure 1: Training & Validation Loss\n",
        "# ============================================================\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "epochs = np.arange(1, len(train_losses) + 1)\n",
        "ax.plot(epochs, train_losses, marker='o', linewidth=2.5,\n",
        "        label='Training Loss', color=colors[0], markersize=6)\n",
        "ax.plot(epochs, val_losses, marker='s', linewidth=2.5,\n",
        "        label='Validation Loss', color=colors[1], markersize=6)\n",
        "\n",
        "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Training Progress', fontsize=14, fontweight='bold', pad=15)\n",
        "ax.legend(frameon=True, loc='best', fontsize=11)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{results_dir}/loss_curves.png\", bbox_inches='tight')\n",
        "print(f\"Saved: {results_dir}/loss_curves.png\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# Figure 2: R² Performance Over Time\n",
        "# ============================================================\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "targets = ['qpd', 'answer_rate', 'retention']\n",
        "labels = {'qpd': 'Questions Per Day', 'answer_rate': 'Answer Rate', 'retention': 'User Retention'}\n",
        "\n",
        "for target in targets:\n",
        "    r2_values = val_metrics_history[target]['r2']\n",
        "    ax.plot(epochs, r2_values, marker='o', linewidth=2.5,\n",
        "            label=labels[target], color=metric_colors[target], markersize=6)\n",
        "\n",
        "ax.axhline(y=0.4, color='gray', linestyle='--', alpha=0.5, linewidth=2, label='Target (0.4)')\n",
        "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('R² Performance by Metric', fontsize=14, fontweight='bold', pad=15)\n",
        "ax.legend(frameon=True, loc='best', fontsize=10)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.set_ylim(bottom=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{results_dir}/r2_over_time.png\", bbox_inches='tight')\n",
        "print(f\"Saved: {results_dir}/r2_over_time.png\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# Figure 3: Final Performance Comparison\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Prepare data\n",
        "metric_names = [labels[t] for t in targets]\n",
        "final_r2 = [val_metrics_history[t]['r2'][-1] for t in targets]\n",
        "final_mae = [val_metrics_history[t]['mae'][-1] for t in targets]\n",
        "\n",
        "# R² comparison\n",
        "ax1 = axes[0]\n",
        "bars = ax1.barh(metric_names, final_r2, color=[metric_colors[t] for t in targets], alpha=0.8)\n",
        "ax1.axvline(x=0.4, color='gray', linestyle='--', linewidth=2, alpha=0.5, label='Target')\n",
        "ax1.set_xlabel('R² Score', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('R² Performance', fontsize=13, fontweight='bold', pad=10)\n",
        "ax1.set_xlim(0, max(final_r2) * 1.2)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--', axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, final_r2)):\n",
        "    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "             f'{val:.3f}', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# MAE comparison\n",
        "ax2 = axes[1]\n",
        "bars = ax2.barh(metric_names, final_mae, color=[metric_colors[t] for t in targets], alpha=0.8)\n",
        "ax2.set_xlabel('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('MAE Performance', fontsize=13, fontweight='bold', pad=10)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--', axis='x')\n",
        "ax2.invert_xaxis()  # Lower is better\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, final_mae)):\n",
        "    ax2.text(val - 0.01, bar.get_y() + bar.get_height()/2,\n",
        "             f'{val:.3f}', va='center', ha='right', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{results_dir}/final_performance.png\", bbox_inches='tight')\n",
        "print(f\"Saved: {results_dir}/final_performance.png\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# Figure 4: Detailed Metric Evolution (3 subplots)\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, (target, ax) in enumerate(zip(targets, axes)):\n",
        "    # Plot MAE and RMSE\n",
        "    mae_values = val_metrics_history[target]['mae']\n",
        "    rmse_values = val_metrics_history[target]['rmse']\n",
        "    r2_values = val_metrics_history[target]['r2']\n",
        "\n",
        "    color = metric_colors[target]\n",
        "\n",
        "    # Primary axis: MAE and RMSE\n",
        "    ax.plot(epochs, mae_values, marker='o', linewidth=2,\n",
        "            label='MAE', color=color, markersize=5)\n",
        "    ax.plot(epochs, rmse_values, marker='s', linewidth=2,\n",
        "            label='RMSE', color=color, alpha=0.6, markersize=5, linestyle='--')\n",
        "\n",
        "    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Error', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(labels[target], fontsize=12, fontweight='bold', pad=10)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.legend(loc='upper left', fontsize=9)\n",
        "\n",
        "    # Secondary axis: R²\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(epochs, r2_values, marker='^', linewidth=2,\n",
        "             label='R²', color='green', alpha=0.7, markersize=5)\n",
        "    ax2.axhline(y=0.4, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax2.set_ylabel('R²', fontsize=11, fontweight='bold', color='green')\n",
        "    ax2.tick_params(axis='y', labelcolor='green')\n",
        "    ax2.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{results_dir}/metric_evolution.png\", bbox_inches='tight')\n",
        "print(f\"Saved: {results_dir}/metric_evolution.png\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# Figure 5: Results Summary Table (as image)\n",
        "# ============================================================\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Prepare table data\n",
        "table_data = []\n",
        "table_data.append(['Metric', 'MAE ↓', 'RMSE ↓', 'R² ↑', 'Normalized MAE'])\n",
        "\n",
        "norm_stats = train_dataset.norm_stats\n",
        "for target in targets:\n",
        "    final_mae = val_metrics_history[target]['mae'][-1]\n",
        "    final_rmse = val_metrics_history[target]['rmse'][-1]\n",
        "    final_r2 = val_metrics_history[target]['r2'][-1]\n",
        "    norm_mae = final_mae / norm_stats[target]['std']\n",
        "\n",
        "    table_data.append([\n",
        "        labels[target],\n",
        "        f'{final_mae:.4f}',\n",
        "        f'{final_rmse:.4f}',\n",
        "        f'{final_r2:.4f}',\n",
        "        f'{norm_mae:.4f}'\n",
        "    ])\n",
        "\n",
        "# Add mean row\n",
        "mean_mae = np.mean([val_metrics_history[t]['mae'][-1] for t in targets])\n",
        "mean_rmse = np.mean([val_metrics_history[t]['rmse'][-1] for t in targets])\n",
        "mean_r2 = np.mean([val_metrics_history[t]['r2'][-1] for t in targets])\n",
        "mean_norm_mae = np.mean([val_metrics_history[t]['mae'][-1] / norm_stats[t]['std'] for t in targets])\n",
        "\n",
        "table_data.append([\n",
        "    'MEAN',\n",
        "    f'{mean_mae:.4f}',\n",
        "    f'{mean_rmse:.4f}',\n",
        "    f'{mean_r2:.4f}',\n",
        "    f'{mean_norm_mae:.4f}'\n",
        "])\n",
        "\n",
        "# Create table\n",
        "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                colWidths=[0.25, 0.15, 0.15, 0.15, 0.2])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(11)\n",
        "table.scale(1, 2)\n",
        "\n",
        "# Style header row\n",
        "for i in range(5):\n",
        "    cell = table[(0, i)]\n",
        "    cell.set_facecolor('#2E86AB')\n",
        "    cell.set_text_props(weight='bold', color='white')\n",
        "\n",
        "# Style mean row\n",
        "for i in range(5):\n",
        "    cell = table[(len(table_data)-1, i)]\n",
        "    cell.set_facecolor('#E8E8E8')\n",
        "    cell.set_text_props(weight='bold')\n",
        "\n",
        "# Alternate row colors\n",
        "for i in range(1, len(table_data)-1):\n",
        "    for j in range(5):\n",
        "        if i % 2 == 0:\n",
        "            table[(i, j)].set_facecolor('#F5F5F5')\n",
        "\n",
        "plt.title('Final Validation Results', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.savefig(f\"{results_dir}/results_table.png\", bbox_inches='tight')\n",
        "print(f\"Saved: {results_dir}/results_table.png\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# Print Summary\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VISUALIZATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nAll figures saved to: {results_dir}/\")\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"  1. loss_curves.png       - Training/validation loss\")\n",
        "print(\"  2. r2_over_time.png      - R² evolution by metric\")\n",
        "print(\"  3. final_performance.png - Final R² and MAE comparison\")\n",
        "print(\"  4. metric_evolution.png  - Detailed metric tracking\")\n",
        "print(\"  5. results_table.png     - Summary table\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# ============================================================\n",
        "# Detailed Text Summary\n",
        "# ============================================================\n",
        "print(\"\\nFINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBest Validation Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Achieved at Epoch: {checkpoint['epoch'] if 'checkpoint' in globals() else 'N/A'}\\n\")\n",
        "\n",
        "print(\"Per-Metric Performance:\")\n",
        "print(f\"{'Metric':<20} {'MAE ↓':<12} {'RMSE ↓':<12} {'R² ↑':<12} {'Norm MAE ↓':<12}\")\n",
        "print(\"-\" * 68)\n",
        "\n",
        "for target in targets:\n",
        "    final_mae = val_metrics_history[target]['mae'][-1]\n",
        "    final_rmse = val_metrics_history[target]['rmse'][-1]\n",
        "    final_r2 = val_metrics_history[target]['r2'][-1]\n",
        "    norm_mae = final_mae / norm_stats[target]['std']\n",
        "\n",
        "    print(f\"{labels[target]:<20} {final_mae:<12.4f} {final_rmse:<12.4f} \"\n",
        "          f\"{final_r2:<12.4f} {norm_mae:<12.4f}\")\n",
        "\n",
        "print(\"-\" * 68)\n",
        "print(f\"{'MEAN':<20} {mean_mae:<12.4f} {mean_rmse:<12.4f} \"\n",
        "      f\"{mean_r2:<12.4f} {mean_norm_mae:<12.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTERPRETATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if mean_r2 > 0.5:\n",
        "    print(\"✓ Strong predictive performance (R² > 0.5)\")\n",
        "elif mean_r2 > 0.4:\n",
        "    print(\"✓ Good predictive performance (R² > 0.4)\")\n",
        "elif mean_r2 > 0.3:\n",
        "    print(\"⚠ Moderate performance (R² > 0.3)\")\n",
        "    print(\"  Consider: Temporal distribution shift (COVID-19 period in validation)\")\n",
        "else:\n",
        "    print(\"⚠ Weak performance (R² < 0.3)\")\n",
        "\n",
        "print(\"\\nPractical Interpretation:\")\n",
        "print(f\"  • Questions/Day:  ±{val_metrics_history['qpd']['mae'][-1]:.2f} questions\")\n",
        "print(f\"  • Answer Rate:    ±{val_metrics_history['answer_rate']['mae'][-1]:.1%}\")\n",
        "print(f\"  • User Retention: ±{val_metrics_history['retention']['mae'][-1]:.1%}\")\n",
        "\n",
        "print(\"\\nNormalized MAE (lower is better):\")\n",
        "for target in targets:\n",
        "    norm_mae = val_metrics_history[target]['mae'][-1] / norm_stats[target]['std']\n",
        "    if norm_mae < 0.5:\n",
        "        status = \"✓ Excellent\"\n",
        "    elif norm_mae < 1.0:\n",
        "        status = \"✓ Good\"\n",
        "    else:\n",
        "        status = \"⚠ Needs improvement\"\n",
        "    print(f\"  • {labels[target]:<17}: {norm_mae:.3f} {status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Save GNN Results"
      ],
      "metadata": {
        "id": "zUHFGffhZo56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save GNN results\n",
        "gnn_results = {\n",
        "    'val_loss': best_val_loss,\n",
        "    'metrics': metrics,  # Final validation metrics\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'val_metrics_history': val_metrics_history\n",
        "}\n",
        "\n",
        "with open(f\"{results_dir}/baseline_gnn/gnn_results.pkl\", 'wb') as f:\n",
        "    pickle.dump(gnn_results, f)"
      ],
      "metadata": {
        "id": "w_TwZX1IaoKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30a5632b244648dc92f840ae50ec2dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e89f84209f64c5ea95431c72de5b4cb",
              "IPY_MODEL_f1bcb0c7458e4b8e8e2bbf0f57209702",
              "IPY_MODEL_df73512824764d378af7492f16cdf44a"
            ],
            "layout": "IPY_MODEL_e34254c196ec4f0d80245cbd49fd2796"
          }
        },
        "0e89f84209f64c5ea95431c72de5b4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08cde1ed291542068785d578c853f8ef",
            "placeholder": "​",
            "style": "IPY_MODEL_5655cf9c43134764b249af108abfbdb6",
            "value": "Training:  14%"
          }
        },
        "f1bcb0c7458e4b8e8e2bbf0f57209702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8487f56ac643432fa267a25d7617d469",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c02e11e183b849e682f284ae988517a1",
            "value": 4
          }
        },
        "df73512824764d378af7492f16cdf44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e23a4692d1fe46ec8c39c7c73e8f5172",
            "placeholder": "​",
            "style": "IPY_MODEL_994a846c44d94cf9b52474aea4e96811",
            "value": " 4/29 [39:53&lt;3:01:15, 435.00s/it, loss=0.8140]"
          }
        },
        "e34254c196ec4f0d80245cbd49fd2796": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08cde1ed291542068785d578c853f8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5655cf9c43134764b249af108abfbdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8487f56ac643432fa267a25d7617d469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02e11e183b849e682f284ae988517a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e23a4692d1fe46ec8c39c7c73e8f5172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "994a846c44d94cf9b52474aea4e96811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}