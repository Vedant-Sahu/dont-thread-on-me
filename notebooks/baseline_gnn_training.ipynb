{"cells":[{"cell_type":"markdown","metadata":{"id":"title"},"source":["# Baseline GNN Training\n","\n","- Loads model from `baseline_gnn.py` file\n","- Starts training from 2014 (skips noisy early years)\n","- Uses 2K stratified samples for fast iteration\n","- Lazy loading with LRU cache\n","- Works both locally and on Colab"]},{"cell_type":"markdown","metadata":{"id":"MXKW_Lj5hHYR"},"source":["## 1. Setup Environment"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mHWpjST0hHYX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba081a0d-ad29-49e6-d0a7-480b69f532d6","executionInfo":{"status":"ok","timestamp":1765506903506,"user_tz":480,"elapsed":9920,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on Google Colab\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n","Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n","Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-geometric\n","Successfully installed torch-geometric-2.7.0\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n"]}],"source":["# Check if running on Colab\n","import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","\n","if IN_COLAB:\n","    print(\"Running on Google Colab\")\n","    !pip install torch-geometric\n","    !pip install python-dateutil\n","else:\n","    print(\"Running locally\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QSbnXaffhHYa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bed6bb45-045a-4741-a580-612d577d5b53","executionInfo":{"status":"ok","timestamp":1765506907629,"user_tz":480,"elapsed":4113,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU available: nvidia a100-sxm4-40gb\n","GPU memory: 42.47 GB\n","Detected Hopper/Ampere GPU → using BF16\n","\n","Final Precision Settings\n","Using device: cuda\n","AMP enabled:  True\n","BF16 enabled: True\n"]}],"source":["# Check GPU availability\n","import torch\n","\n","# Default values\n","USE_BF16 = False\n","USE_AMP = torch.cuda.is_available()  # AMP works only on CUDA\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    gpu_name = torch.cuda.get_device_name(0).lower()\n","    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n","\n","    print(f\"GPU available: {gpu_name}\")\n","    print(f\"GPU memory: {gpu_mem:.2f} GB\")\n","\n","    # Precision selection based on GPU family\n","    if any(key in gpu_name for key in ['b200', 'b100', 'blackwell']):\n","        print(\"Detected Blackwell GPU → using BF16 for optimal performance\")\n","        USE_BF16 = True\n","\n","    elif any(key in gpu_name for key in ['h100', 'a100']):\n","        print(\"Detected Hopper/Ampere GPU → using BF16\")\n","        USE_BF16 = True\n","\n","    else:\n","        print(\"Standard CUDA GPU detected → using FP16 (AMP)\")\n","        USE_BF16 = False\n","\n","else:\n","    device = torch.device('cpu')\n","    print(\"No GPU available → using CPU\")\n","    USE_BF16 = False\n","    USE_AMP = False\n","\n","print(\"\\nFinal Precision Settings\")\n","print(f\"Using device: {device}\")\n","print(f\"AMP enabled:  {USE_AMP}\")\n","print(f\"BF16 enabled: {USE_BF16}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"q0M-vbe1hHYb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b42132e3-1f57-4e7c-d48f-4f4f6173ce83","executionInfo":{"status":"ok","timestamp":1765506922284,"user_tz":480,"elapsed":14653,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted\n","Project directory: /content/drive/MyDrive/dont-thread-on-me\n","Graph directory: /content/drive/MyDrive/dont-thread-on-me/data/processed/graphs\n"]}],"source":["# Setup paths based on environment\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    PROJECT_DIR = '/content/drive/MyDrive/dont-thread-on-me'\n","    GRAPH_DIR = f'{PROJECT_DIR}/data/processed/graphs'\n","\n","    # Add project directory to path for imports\n","    if PROJECT_DIR not in sys.path:\n","        sys.path.append(PROJECT_DIR)\n","\n","    print(f\"Google Drive mounted\")\n","    print(f\"Project directory: {PROJECT_DIR}\")\n","\n","else:\n","    PROJECT_DIR = '..'\n","    GRAPH_DIR = '../data/processed/graphs'\n","\n","    if PROJECT_DIR not in sys.path:\n","        sys.path.append(PROJECT_DIR)\n","\n","    print(f\"Running locally\")\n","    print(f\"Project directory: {PROJECT_DIR}\")\n","\n","print(f\"Graph directory: {GRAPH_DIR}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"verify_data","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ebc1769-ddaa-43dc-d03d-2a59e288d495","executionInfo":{"status":"ok","timestamp":1765506927659,"user_tz":480,"elapsed":5373,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 177 communities\n","  - 3dprinting.stackexchange.com: 98 monthly graphs\n","  - academia.stackexchange.com: 147 monthly graphs\n","  - ai.stackexchange.com: 91 monthly graphs\n","  - android.stackexchange.com: 174 monthly graphs\n","  - anime.stackexchange.com: 135 monthly graphs\n","  ... and 172 more\n"]}],"source":["# Verify data directory exists\n","from pathlib import Path\n","\n","graph_dir = Path(GRAPH_DIR)\n","\n","if not graph_dir.exists():\n","    print(f\"ERROR: Graph directory not found: {graph_dir}\")\n","    print(f\"\\nPlease upload your processed graphs to:\")\n","    print(f\"  {GRAPH_DIR}\")\n","else:\n","    communities = [d.name for d in graph_dir.iterdir() if d.is_dir()]\n","    print(f\"Found {len(communities)} communities\")\n","\n","    # Show first few\n","    for comm in sorted(communities)[:5]:\n","        comm_path = graph_dir / comm\n","        n_graphs = len(list(comm_path.glob('*.pt')))\n","        print(f\"  - {comm}: {n_graphs} monthly graphs\")\n","\n","    if len(communities) > 5:\n","        print(f\"  ... and {len(communities) - 5} more\")"]},{"cell_type":"markdown","metadata":{"id":"import_model"},"source":["## 2. Import Model from baseline_gnn.py\n","\n","**Note:** Make sure `baseline_gnn.py` is uploaded to your project directory:\n","- Colab: `/content/drive/MyDrive/dont-thread-on-me/src/models/baseline_gnn.py`\n","- Local: `../src/models/baseline_gnn.py`"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"import_model_code","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a94e64d0-4d99-4a6b-bc40-947f142af179","executionInfo":{"status":"ok","timestamp":1765506937439,"user_tz":480,"elapsed":9775,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully imported model from baseline_gnn.py\n"]}],"source":["# Try to import the model\n","try:\n","    from src.models.baseline_gnn import create_model\n","    print(\"Successfully imported model from baseline_gnn.py\")\n","except ImportError as e:\n","    print(\"Failed to import model\")\n","    print(f\"\\nError: {e}\")\n","    print(f\"\\nPlease make sure baseline_gnn.py exists at:\")\n","    if IN_COLAB:\n","        print(f\"  /content/drive/MyDrive/dont-thread-on-me/src/models/baseline_gnn.py\")\n","    else:\n","        print(f\"  ../src/models/baseline_gnn.py\")\n","    raise"]},{"cell_type":"markdown","metadata":{"id":"dataset_section"},"source":["## 3. Define Optimized Dataset\n","\n","- **No pre-caching** - starts training immediately\n","- **LRU cache** - automatically caches frequently accessed graphs\n","- **Stratified sampling** - representative samples across communities\n","- **2014 start date** - skips noisy early Stack Exchange years"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dataset_imports","executionInfo":{"status":"ok","timestamp":1765506937449,"user_tz":480,"elapsed":5,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[],"source":["# Dataset imports\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","from typing import List, Dict\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","from functools import lru_cache\n","from collections import defaultdict\n","import random\n","import numpy as np\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"optimized_dataset","executionInfo":{"status":"ok","timestamp":1765506937463,"user_tz":480,"elapsed":6,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[],"source":["class FastTemporalDataset(Dataset):\n","    \"\"\"\n","    Optimized dataset with lazy loading + LRU cache + target normalization.\n","\n","    - No slow pre-caching step (starts training immediately)\n","    - LRU cache automatically keeps hot graphs in memory\n","    - Memory efficient (only caches ~2000 graphs, not all)\n","    - Stratified sampling for representative coverage\n","    - Starts from 2014 (skips noisy early years)\n","    - Target normalization: Standardizes all targets to mean=0, std=1\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        graph_dir: Path,\n","        split: str = 'train',\n","        sequence_length: int = 12,\n","        prediction_horizon: int = 6,\n","        max_samples: int = None,\n","        cache_size: int = 2000,\n","        stratified_sample: bool = True\n","    ):\n","        self.graph_dir = Path(graph_dir)\n","        self.split = split\n","        self.sequence_length = sequence_length\n","        self.prediction_horizon = prediction_horizon\n","\n","        # Temporal splits\n","        self.split_ranges = {\n","            'train': ('2014-01', '2020-06'),  # Predict through 2020-12\n","            'val':   ('2020-07', '2022-09'),  # Predict through 2023-03\n","            'test':  ('2022-10', '2023-09')   # Predict through 2024-03\n","        }\n","\n","        # Build sample index\n","        print(f\"Building {split} sample index...\")\n","        self.samples = self._build_sample_index()\n","        print(f\"  Found {len(self.samples)} potential samples\")\n","\n","        # Apply sampling strategy\n","        if max_samples and len(self.samples) > max_samples:\n","            if stratified_sample:\n","                print(f\"  Applying stratified sampling to select {max_samples} samples...\")\n","                self.samples = self._stratified_sample(max_samples)\n","            else:\n","                print(f\"  Randomly sampling {max_samples} samples...\")\n","                random.shuffle(self.samples)\n","                self.samples = self.samples[:max_samples]\n","\n","        print(f\"{split.upper()} Dataset: {len(self.samples)} samples\")\n","\n","        # Initialize normalization stats (will be computed for train, loaded for val/test)\n","        self.norm_stats = None\n","\n","        # Create LRU cached loader for individual monthly graphs\n","        @lru_cache(maxsize=cache_size)\n","        def _cached_load(community: str, month: str):\n","            path = self.graph_dir / community / f\"{month}.pt\"\n","            return torch.load(path, weights_only=False, map_location='cpu')\n","\n","        self._load_graph = _cached_load\n","        self._access_count = 0\n","\n","    def compute_normalization_stats(self):\n","        \"\"\"\n","        Compute mean and std for each target metric from training data.\n","\n","        IMPORTANT: Only call this on the training dataset.\n","        Validation and test datasets will use these same statistics.\n","        \"\"\"\n","        print(\"\\n\" + \"=\"*70)\n","        print(\"Computing target normalization statistics\")\n","        print(\"=\"*70)\n","        print(\"Sampling up to 1000 examples to compute statistics...\")\n","\n","        all_targets = {\n","            'qpd': [],\n","            'answer_rate': [],\n","            'retention': []\n","        }\n","\n","        # Sample up to 1000 examples for statistics\n","        sample_size = min(1000, len(self.samples))\n","        sample_indices = np.random.choice(len(self.samples), sample_size, replace=False)\n","\n","        for idx in tqdm(sample_indices, desc='Sampling targets'):\n","            sample = self.samples[idx]\n","            target_graph = self._load_graph(sample['community'], sample['target_month'])\n","            targets = target_graph.y\n","\n","            for key in all_targets:\n","                if key in targets:\n","                    all_targets[key].append(float(targets[key]))\n","\n","        # Compute mean and std for each metric\n","        self.norm_stats = {}\n","        for key in all_targets:\n","            if len(all_targets[key]) > 0:\n","                values = np.array(all_targets[key])\n","                self.norm_stats[key] = {\n","                    'mean': float(values.mean()),\n","                    'std': float(values.std() + 1e-8)  # Add small constant to avoid division by zero\n","                }\n","\n","        print(\"\\nNormalization statistics (computed from training data):\")\n","        print(f\"{'Metric':<15} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n","        print(\"-\" * 63)\n","\n","        for key in all_targets:\n","            if key in self.norm_stats:\n","                values = np.array(all_targets[key])\n","                print(f\"{key:<15} {self.norm_stats[key]['mean']:<12.4f} \"\n","                      f\"{self.norm_stats[key]['std']:<12.4f} \"\n","                      f\"{values.min():<12.4f} {values.max():<12.4f}\")\n","\n","        print(\"=\"*70)\n","        print(\"  Normalization statistics computed\")\n","        print(\"  These will be used to standardize all targets during training\")\n","        print(\"  Val/test datasets will use these SAME statistics (no data leakage)\")\n","        print(\"=\"*70 + \"\\n\")\n","\n","        return self.norm_stats\n","\n","    def _build_sample_index(self) -> List[Dict]:\n","        \"\"\"Build index of all valid temporal sequences.\"\"\"\n","        samples = []\n","        start_month, end_month = self.split_ranges[self.split]\n","        min_graphs = self.sequence_length + self.prediction_horizon\n","\n","        for community_dir in sorted(self.graph_dir.iterdir()):\n","            if not community_dir.is_dir():\n","                continue\n","\n","            available_months = sorted([f.stem for f in community_dir.glob('*.pt')])\n","            if len(available_months) < min_graphs:\n","                continue\n","\n","            for i, month_t in enumerate(available_months):\n","                # Check if month is in split range\n","                if not (start_month <= month_t <= end_month):\n","                    continue\n","                if i < self.sequence_length - 1:\n","                    continue\n","\n","                target_idx = i + self.prediction_horizon\n","                if target_idx >= len(available_months):\n","                    continue\n","\n","                seq_start = i - self.sequence_length + 1\n","                seq_months = available_months[seq_start:i+1]\n","                target_month = available_months[target_idx]\n","\n","                # Check temporal consistency\n","                if self._is_consecutive(seq_months, target_month):\n","                    samples.append({\n","                        'community': community_dir.name,\n","                        'sequence_months': seq_months,\n","                        'target_month': target_month\n","                    })\n","\n","        return samples\n","\n","    def _is_consecutive(self, seq_months, target_month):\n","        \"\"\"Check if months form consecutive sequence.\"\"\"\n","        try:\n","            dates = [datetime.strptime(m, '%Y-%m') for m in seq_months]\n","            for i in range(1, len(dates)):\n","                if dates[i] != dates[i-1] + relativedelta(months=1):\n","                    return False\n","            target_date = datetime.strptime(target_month, '%Y-%m')\n","            expected = dates[-1] + relativedelta(months=self.prediction_horizon)\n","            return target_date == expected\n","        except:\n","            return False\n","\n","    def _stratified_sample(self, n_samples: int) -> List[Dict]:\n","        \"\"\"\n","        Create stratified sample across communities.\n","        Ensures representative distribution across small/medium/large communities.\n","        \"\"\"\n","        # Group by community\n","        by_community = defaultdict(list)\n","        for sample in self.samples:\n","            by_community[sample['community']].append(sample)\n","\n","        # Sample proportionally from each community\n","        samples_per_community = max(1, n_samples // len(by_community))\n","        selected = []\n","\n","        for community, comm_samples in by_community.items():\n","            n = min(samples_per_community, len(comm_samples))\n","            selected.extend(random.sample(comm_samples, n))\n","\n","        # Trim to exact size and shuffle\n","        random.shuffle(selected)\n","        return selected[:n_samples]\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","        community = sample['community']\n","\n","        # Load sequence using cached loader\n","        graphs = [\n","            self._load_graph(community, month)\n","            for month in sample['sequence_months']\n","        ]\n","\n","        target_graph = self._load_graph(community, sample['target_month'])\n","        targets = target_graph.y\n","\n","        # Apply standardization if statistics are available\n","        if self.norm_stats is not None:\n","            targets_standardized = {}\n","            for key in targets:\n","                if key in self.norm_stats:\n","                    # Z-score normalization: (x - mean) / std\n","                    targets_standardized[key] = (\n","                        (targets[key] - self.norm_stats[key]['mean']) /\n","                        self.norm_stats[key]['std']\n","                    )\n","                else:\n","                    targets_standardized[key] = targets[key]\n","            targets = targets_standardized\n","\n","        # Periodic cache statistics (every 1000 accesses)\n","        self._access_count += 1\n","        if self._access_count % 1000 == 0:\n","            cache_info = self._load_graph.cache_info()\n","            if cache_info.hits + cache_info.misses > 0:\n","                hit_rate = cache_info.hits / (cache_info.hits + cache_info.misses)\n","                print(f\"  Cache: {cache_info.currsize}/{cache_info.maxsize} graphs | Hit rate: {hit_rate:.1%}\")\n","\n","        return graphs, targets\n","\n","    def get_cache_stats(self):\n","        \"\"\"Get current cache statistics.\"\"\"\n","        return self._load_graph.cache_info()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"collate_fn","executionInfo":{"status":"ok","timestamp":1765506937473,"user_tz":480,"elapsed":5,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[],"source":["def collate_fn(batch):\n","    \"\"\"Collate function for batching.\"\"\"\n","    batch_graphs = []\n","    batch_targets = {'qpd': [], 'answer_rate': [], 'retention': []}\n","\n","    for graphs, targets in batch:\n","        batch_graphs.append(graphs)\n","        for key in batch_targets:\n","            batch_targets[key].append(targets[key])\n","\n","    for key in batch_targets:\n","        batch_targets[key] = torch.tensor(batch_targets[key], dtype=torch.float32)\n","\n","    return batch_graphs, batch_targets"]},{"cell_type":"markdown","metadata":{"id":"create_datasets"},"source":["## 4. Load Saved Dataset Configuration\n","\n","**Configuration:**\n","- 2,000 training samples (stratified across communities)\n","- 500 validation samples\n","- Starts from 2014 (skips 2008-2013 noisy data)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"create_datasets_code","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb903531-99ec-405c-f4e5-afc6c64428cd","executionInfo":{"status":"ok","timestamp":1765506938012,"user_tz":480,"elapsed":532,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","Loaded Temporal GNN Training Configuration\n","======================================================================\n","Training samples:   1834\n","Validation samples: 338\n","Normalization stats: ['qpd', 'answer_rate', 'retention']\n","======================================================================\n"]}],"source":["# Dataset imports\n","import pickle\n","\n","config_path = f\"{PROJECT_DIR}/results/baseline_config.pkl\"\n","with open(config_path, 'rb') as f:\n","    config = pickle.load(f)\n","\n","print(\"=\"*70)\n","print(\"Loaded Temporal GNN Training Configuration\")\n","print(\"=\"*70)\n","print(f\"Training samples:   {len(config['train_samples'])}\")\n","print(f\"Validation samples: {len(config['val_samples'])}\")\n","print(f\"Normalization stats: {list(config['norm_stats'].keys())}\")\n","print(\"=\"*70)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"compute_normalization","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765506965620,"user_tz":480,"elapsed":27604,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"b1afdedb-46b3-469a-dad6-5b102ff415cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building train sample index...\n","  Found 11020 potential samples\n","TRAIN Dataset: 11020 samples\n","Building val sample index...\n","  Found 4471 potential samples\n","VAL Dataset: 4471 samples\n","\n","Datasets configured with identical samples and normalization\n","Train dataset size: 1834\n","Val dataset size: 338\n"]}],"source":["# Create datasets\n","train_dataset = FastTemporalDataset(\n","    graph_dir=GRAPH_DIR,\n","    split='train',\n","    max_samples=None,\n","    stratified_sample=False,\n","    cache_size=10000\n",")\n","\n","val_dataset = FastTemporalDataset(\n","    graph_dir=GRAPH_DIR,\n","    split='val',\n","    max_samples=None,\n","    stratified_sample=False,\n","    cache_size=10000\n",")\n","\n","# Now replace samples\n","train_dataset.samples = config['train_samples']\n","val_dataset.samples = config['val_samples']\n","train_dataset.norm_stats = config['norm_stats']\n","val_dataset.norm_stats = config['norm_stats']\n","\n","print(\"\\nDatasets configured with identical samples and normalization\")\n","print(f\"Train dataset size: {len(train_dataset.samples)}\")\n","print(f\"Val dataset size: {len(val_dataset.samples)}\")"]},{"cell_type":"code","source":["from pathlib import Path\n","from tqdm import tqdm\n","from functools import lru_cache\n","import torch\n","import zipfile\n","import shutil\n","from torch.utils.data import DataLoader\n","from torch_geometric.data import Batch\n","\n","# -------------------------------\n","# Paths\n","# -------------------------------\n","DRIVE_ZIP_PATH = \"/content/drive/MyDrive/dont-thread-on-me/data/processed/graphs.zip\"\n","LOCAL_GRAPH_DIR = Path(\"/content/graphs\")  # Local extraction folder\n","\n","# -------------------------------\n","# Copy and extract zip\n","# -------------------------------\n","shutil.copy2(DRIVE_ZIP_PATH, \"/content/graphs.zip\")\n","\n","with zipfile.ZipFile(\"/content/graphs.zip\", 'r') as zip_ref:\n","    zip_ref.extractall(LOCAL_GRAPH_DIR)\n","\n","print(f\"Graphs extracted to {LOCAL_GRAPH_DIR}\")\n","\n","# -------------------------------\n","# Helper: ensure CPU tensors\n","# -------------------------------\n","def graph_to_cpu(graph):\n","    for key, value in graph.items():\n","        if torch.is_tensor(value):\n","            graph[key] = value.cpu()\n","    return graph\n","\n","# -------------------------------\n","# LRU cached loader\n","# -------------------------------\n","@lru_cache(maxsize=None)\n","def _cached_load(community: str, month: str):\n","    path = LOCAL_GRAPH_DIR / \"graphs\" / community / f\"{month}.pt\"\n","    if not path.exists():\n","        raise FileNotFoundError(f\"Graph file not found: {path}\")\n","    g = torch.load(path, map_location='cpu', weights_only=False)  # load on CPU\n","    return graph_to_cpu(g)  # ensure all tensors are CPU\n","\n","# Attach to datasets\n","train_dataset._load_graph = _cached_load\n","val_dataset._load_graph = _cached_load\n","\n","# -------------------------------\n","# Collect unique graphs and pre-load\n","# -------------------------------\n","unique_graphs = set()\n","for sample in train_dataset.samples + val_dataset.samples:\n","    community = sample['community']\n","    for month in sample['sequence_months']:\n","        unique_graphs.add((str(community), str(month)))\n","    unique_graphs.add((str(community), str(sample['target_month'])))\n","\n","print(f\"Pre-loading {len(unique_graphs)} unique graphs into cache...\")\n","\n","for community, month in tqdm(unique_graphs, desc=\"Caching graphs\"):\n","    try:\n","        _ = _cached_load(community, month)\n","    except FileNotFoundError:\n","        print(f\"Warning: missing graph {community}/{month}.pt\")\n","\n","# -------------------------------\n","# Verify cache hit rate\n","# -------------------------------\n","cache_info = _cached_load.cache_info()\n","total = cache_info.hits + cache_info.misses\n","hit_rate = cache_info.hits / total if total > 0 else 0.0\n","print(f\"\\nCache populated: {cache_info.currsize} items | Hit rate: {hit_rate:.1%} | maxsize: {'unbounded' if cache_info.maxsize is None else cache_info.maxsize}\")\n","\n","# -------------------------------\n","# DataLoaders\n","# -------------------------------\n","BATCH_SIZE = 64\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=0,\n","    pin_memory=False  # must be False for PyG Data objects\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=collate_fn,\n","    num_workers=0,\n","    pin_memory=False\n",")\n","\n","print(f\"DataLoaders created | Batch size: {BATCH_SIZE} | Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWA4V22v2QEX","executionInfo":{"status":"ok","timestamp":1765507006563,"user_tz":480,"elapsed":40766,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"d4a48bd8-1988-4004-fae9-f20e1586d410"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Graphs extracted to /content/graphs\n","Pre-loading 13965 unique graphs into cache...\n"]},{"output_type":"stream","name":"stderr","text":["Caching graphs: 100%|██████████| 13965/13965 [00:16<00:00, 834.47it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Cache populated: 13965 items | Hit rate: 0.0% | maxsize: unbounded\n","DataLoaders created | Batch size: 64 | Train batches: 29 | Val batches: 6\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# Second pass to check cache hits\n","hits_before = _cached_load.cache_info().hits\n","\n","print(\"Verifying cache hit rate with a second pass...\")\n","for community, month in tqdm(unique_graphs, desc=\"Checking cache hits\"):\n","    _ = _cached_load(str(community), str(month))  # access exactly same way as pre-loading\n","\n","hits_after = _cached_load.cache_info().hits\n","total_hits = hits_after - hits_before\n","total_accesses = len(unique_graphs)\n","\n","hit_rate = total_hits / total_accesses\n","print(f\"\\nCache verification:\")\n","print(f\"  Total graphs accessed: {total_accesses}\")\n","print(f\"  Cache hits in second pass: {total_hits}\")\n","print(f\"  Hit rate: {hit_rate:.1%}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ss4TZGN63Eom","executionInfo":{"status":"ok","timestamp":1765507006595,"user_tz":480,"elapsed":28,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"6960818b-7c70-4b1e-a408-387edaa638ca"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Verifying cache hit rate with a second pass...\n"]},{"output_type":"stream","name":"stderr","text":["Checking cache hits: 100%|██████████| 13965/13965 [00:00<00:00, 1111936.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Cache verification:\n","  Total graphs accessed: 13965\n","  Cache hits in second pass: 13965\n","  Hit rate: 100.0%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Inspect a sample to verify data structure\n","if len(train_dataset) > 0:\n","    print(\"\\nInspecting first sample...\")\n","    sample_graphs, sample_targets = train_dataset[0]\n","    print(f\"  Sequence length: {len(sample_graphs)} monthly graphs\")\n","\n","    g = sample_graphs[0]\n","    print(f\"\\n  Graph structure:\")\n","    print(f\"    Users: {g['user'].x.shape}\")\n","    print(f\"    Tags: {g['tag'].x.shape}\")\n","\n","    print(f\"\\n  Target metrics:\")\n","    for key, value in sample_targets.items():\n","        print(f\"    {key}: {value:.4f}\")"],"metadata":{"id":"6pebyuEgIYZY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765507006620,"user_tz":480,"elapsed":21,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"9a683fa5-8e19-4d35-de9f-bf977938e19f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Inspecting first sample...\n","  Sequence length: 12 monthly graphs\n","\n","  Graph structure:\n","    Users: torch.Size([228, 5])\n","    Tags: torch.Size([283, 7])\n","\n","  Target metrics:\n","    qpd: -0.2391\n","    answer_rate: -1.8552\n","    retention: -1.6198\n","    growth: -0.3421\n"]}]},{"cell_type":"markdown","metadata":{"id":"create_model_section"},"source":["## 5. Create Model\n","\n","Using the model imported from `baseline_gnn.py`"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"get_feature_dims","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765507006637,"user_tz":480,"elapsed":13,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"106d1c23-7e44-400a-d193-2a20096984f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Feature dimensions:\n","  User features: 5\n","  Tag features: 7\n"]}],"source":["# Get feature dimensions from data\n","if len(train_dataset) > 0:\n","    sample_graphs, _ = train_dataset[0]\n","    USER_FEAT_DIM = sample_graphs[0]['user'].x.shape[1]\n","    TAG_FEAT_DIM = sample_graphs[0]['tag'].x.shape[1]\n","else:\n","    USER_FEAT_DIM = 5\n","    TAG_FEAT_DIM = 7\n","\n","print(f\"Feature dimensions:\")\n","print(f\"  User features: {USER_FEAT_DIM}\")\n","print(f\"  Tag features: {TAG_FEAT_DIM}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"create_model_code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765507006844,"user_tz":480,"elapsed":204,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"fda4d797-2b34-4714-9d5a-0dc410674f5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model created and moved to cuda\n","\n","Model Architecture:\n","  Hidden dimension: 96\n","  Graph conv layers: 2\n","  Dropout: 0.3\n","\n","Parameters:\n","  Total: 113,115\n","  Trainable: 113,115\n"]}],"source":["# Model configuration (optimized for 2K samples)\n","model = create_model(\n","    user_feat_dim=USER_FEAT_DIM,\n","    tag_feat_dim=TAG_FEAT_DIM,\n","    hidden_dim=96,           # Reduced for initial training\n","    num_conv_layers=2,       # Reduced to prevent overfitting\n","    dropout=0.3,             # Higher for regularization with limited data\n","    batched=True\n",")\n","\n","model = model.to(device)\n","\n","# Count parameters\n","n_params = sum(p.numel() for p in model.parameters())\n","n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"\\nModel created and moved to {device}\")\n","print(f\"\\nModel Architecture:\")\n","print(f\"  Hidden dimension: 96\")\n","print(f\"  Graph conv layers: 2\")\n","print(f\"  Dropout: 0.3\")\n","print(f\"\\nParameters:\")\n","print(f\"  Total: {n_params:,}\")\n","print(f\"  Trainable: {n_trainable:,}\")"]},{"cell_type":"markdown","metadata":{"id":"training_section"},"source":["## 6. Training Setup"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"training_config","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765507006876,"user_tz":480,"elapsed":25,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"5c6137a4-8c86-4848-da6b-775a4c874f87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training functions defined\n"]}],"source":["def move_to_device(batch_graphs, device):\n","    \"\"\"Move graphs to device efficiently.\"\"\"\n","    return [[g.to(device, non_blocking=True) for g in graphs] for graphs in batch_graphs]\n","\n","def train_epoch(model, loader, optimizer, criterion, device, scaler, use_amp, scheduler=None, max_grad_norm=1.0):\n","    \"\"\"Train for one epoch.\"\"\"\n","    model.train()\n","    total_loss = 0.0\n","    n_batches = 0\n","\n","    amp_dtype = torch.bfloat16 if USE_BF16 else torch.float16\n","\n","    pbar = tqdm(loader, desc='Training')\n","    for batch_graphs, batch_targets in pbar:\n","        batch_graphs = move_to_device(batch_graphs, device)\n","        batch_targets = {k: v.to(device, non_blocking=True) for k, v in batch_targets.items()}\n","\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        with torch.amp.autocast(device_type='cuda', enabled=use_amp, dtype=amp_dtype):\n","            predictions = model(batch_graphs)\n","\n","            # Simple loss (targets are already standardized!)\n","            loss = (\n","                2.0 * criterion(predictions['qpd'], batch_targets['qpd']) +\n","                0.5 * criterion(predictions['answer_rate'], batch_targets['answer_rate']) +\n","                2.0 * criterion(predictions['retention'], batch_targets['retention'])\n","            ) / 4.5\n","\n","        if use_amp and not USE_BF16:\n","          scaler.scale(loss).backward()\n","\n","          # unscale before clipping\n","          scaler.unscale_(optimizer)\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","          scaler.step(optimizer)\n","          scaler.update()\n","\n","        else:\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","          optimizer.step()\n","\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        total_loss += loss.item()\n","        n_batches += 1\n","        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","    return total_loss / max(n_batches, 1)\n","\n","\n","@torch.no_grad()\n","def evaluate(model, loader, criterion, device, use_amp, norm_stats):\n","    \"\"\"\n","    Evaluate model with INVERSE TRANSFORMATION.\n","\n","    IMPORTANT: Metrics are computed on ORIGINAL scale (after denormalization).\n","    This is standard practice - allows interpretation and comparison to baselines.\n","    \"\"\"\n","    model.eval()\n","    total_loss = 0.0  # Loss on standardized scale\n","    n_batches = 0\n","\n","    all_preds = {'qpd': [], 'answer_rate': [], 'retention': []}\n","    all_targets = {'qpd': [], 'answer_rate': [], 'retention': []}\n","\n","    amp_dtype = torch.bfloat16 if USE_BF16 else torch.float16\n","\n","    for batch_graphs, batch_targets in tqdm(loader, desc='Evaluating', leave=False):\n","        batch_graphs = move_to_device(batch_graphs, device)\n","        batch_targets = {k: v.to(device, non_blocking=True) for k, v in batch_targets.items()}\n","\n","        with torch.amp.autocast('cuda', enabled=use_amp, dtype=amp_dtype):\n","            predictions = model(batch_graphs)\n","\n","            # Compute loss on standardized scale\n","            loss = (\n","                2.0 * criterion(predictions['qpd'], batch_targets['qpd']) +\n","                0.5 * criterion(predictions['answer_rate'], batch_targets['answer_rate']) +\n","                2.0 * criterion(predictions['retention'], batch_targets['retention'])\n","            ) / 4.5\n","\n","        total_loss += loss.item()\n","        n_batches += 1\n","\n","        # INVERSE TRANSFORM back to original scale for metrics\n","        for key in predictions:\n","            if key in norm_stats:\n","                # Inverse z-score: x = z * std + mean\n","                preds_original = (\n","                    predictions[key].float().cpu().numpy() * norm_stats[key]['std'] +\n","                    norm_stats[key]['mean']\n","                )\n","                targets_original = (\n","                    batch_targets[key].float().cpu().numpy() * norm_stats[key]['std'] +\n","                    norm_stats[key]['mean']\n","                )\n","            else:\n","                preds_original = predictions[key].float().cpu().numpy()\n","                targets_original = batch_targets[key].float().cpu().numpy()\n","\n","            all_preds[key].extend(preds_original)\n","            all_targets[key].extend(targets_original)\n","\n","    # Compute metrics on ORIGINAL scale (after inverse transform)\n","    metrics = {}\n","    for key in all_preds:\n","        preds = np.array(all_preds[key])\n","        targets = np.array(all_targets[key])\n","\n","        # MAE, R², RMSE on original scale\n","        mae = np.mean(np.abs(preds - targets))\n","        ss_res = np.sum((targets - preds) ** 2)\n","        ss_tot = np.sum((targets - targets.mean()) ** 2)\n","        r2 = 1 - ss_res / (ss_tot + 1e-8)\n","        rmse = np.sqrt(np.mean((targets - preds) ** 2))\n","\n","        metrics[key] = {'mae': mae, 'r2': r2, 'rmse': rmse}\n","\n","    return total_loss / max(n_batches, 1), metrics\n","\n","print(\"Training functions defined\")"]},{"cell_type":"code","source":["# Training configuration\n","LEARNING_RATE = 1e-3\n","NUM_EPOCHS = 50  # Should be sufficient with 2K samples\n","PATIENCE = 10     # Early stopping patience\n","USE_AMP = torch.cuda.is_available()\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n","criterion = nn.MSELoss()\n","scaler = torch.amp.GradScaler(enabled=USE_AMP and not USE_BF16)\n","\n","# Learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',\n","    factor=0.5,\n","    patience=5,\n","    min_lr=1e-6\n",")\n","\n","print(\"Training Configuration:\")\n","print(f\"  Device: {device}\")\n","print(f\"  AMP: {USE_AMP}\")\n","print(f\"  Dtype: {'bfloat16' if USE_BF16 else 'float16' if USE_AMP else 'float32'}\")\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {LEARNING_RATE}\")\n","print(f\"  Epochs: {NUM_EPOCHS}\")\n","print(f\"  Early stopping patience: {PATIENCE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_9zGvXbRariI","executionInfo":{"status":"ok","timestamp":1765507006907,"user_tz":480,"elapsed":21,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"78d68b22-01f5-4269-e94d-bc558f4bb169"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Configuration:\n","  Device: cuda\n","  AMP: True\n","  Dtype: bfloat16\n","  Batch size: 64\n","  Learning rate: 0.001\n","  Epochs: 50\n","  Early stopping patience: 10\n"]}]},{"cell_type":"markdown","metadata":{"id":"quick_test_section"},"source":["## 7. Quick Test (Single Batch)\n","\n","Verify everything works before full training"]},{"cell_type":"code","source":["# ============================================================\n","# PERFORMANCE BENCHMARK - A100 GPU\n","# ============================================================\n","\n","import time\n","import numpy as np\n","\n","print(\"=\"*70)\n","print(\"PERFORMANCE BENCHMARK - Testing on A100 GPU\")\n","print(\"=\"*70)\n","\n","if len(train_loader) == 0:\n","    print(\"No data in train_loader\")\n","else:\n","    # Get test batch\n","    batch_graphs, batch_targets = next(iter(train_loader))\n","    batch_graphs = move_to_device(batch_graphs, device)\n","    batch_targets = {k: v.to(device, non_blocking=True) for k, v in batch_targets.items()}\n","\n","    amp_dtype = torch.bfloat16 if USE_BF16 else torch.float16\n","\n","    # --------------------------------------------------------\n","    # 1. WARMUP (important for accurate timing!)\n","    # --------------------------------------------------------\n","    print(\"\\n1. Warming up GPU (JIT compilation, caching)...\")\n","    for _ in range(3):\n","        with torch.amp.autocast('cuda', enabled=USE_AMP, dtype=amp_dtype):\n","            preds = model(batch_graphs)\n","            loss = (\n","                2.0 * criterion(preds['qpd'], batch_targets['qpd']) +\n","                0.5 * criterion(preds['answer_rate'], batch_targets['answer_rate']) +\n","                2.0 * criterion(preds['retention'], batch_targets['retention'])\n","            ) / 4.5\n","        loss.backward()\n","        optimizer.zero_grad()\n","\n","    torch.cuda.synchronize()\n","    print(\"Warmup complete\")\n","\n","    # --------------------------------------------------------\n","    # 2. FORWARD PASS ONLY (inference speed)\n","    # --------------------------------------------------------\n","    print(\"\\n2. Testing forward pass (inference)...\")\n","    forward_times = []\n","\n","    for i in range(5):\n","        torch.cuda.synchronize()\n","        start = time.time()\n","\n","        with torch.amp.autocast('cuda', enabled=USE_AMP, dtype=amp_dtype):\n","            preds = model(batch_graphs)\n","            loss = (\n","                2.0 * criterion(preds['qpd'], batch_targets['qpd']) +\n","                0.5 * criterion(preds['answer_rate'], batch_targets['answer_rate']) +\n","                2.0 * criterion(preds['retention'], batch_targets['retention'])\n","            ) / 4.5\n","\n","        torch.cuda.synchronize()\n","        elapsed = time.time() - start\n","        forward_times.append(elapsed)\n","        print(f\"   Run {i+1}: {elapsed:.3f}s\")\n","\n","    avg_forward = np.mean(forward_times)\n","    std_forward = np.std(forward_times)\n","\n","    # --------------------------------------------------------\n","    # 3. FORWARD + BACKWARD PASS (training speed)\n","    # --------------------------------------------------------\n","    print(\"\\n3. Testing forward + backward pass (training)...\")\n","    train_times = []\n","\n","    for i in range(5):\n","        optimizer.zero_grad()\n","\n","        torch.cuda.synchronize()\n","        start = time.time()\n","\n","        with torch.amp.autocast('cuda', enabled=USE_AMP, dtype=amp_dtype):\n","            preds = model(batch_graphs)\n","            loss = (\n","                2.0 * criterion(preds['qpd'], batch_targets['qpd']) +\n","                0.5 * criterion(preds['answer_rate'], batch_targets['answer_rate']) +\n","                2.0 * criterion(preds['retention'], batch_targets['retention'])\n","            ) / 4.5\n","\n","        if USE_AMP:\n","            scaler.scale(loss).backward()\n","        else:\n","            loss.backward()\n","\n","        torch.cuda.synchronize()\n","        elapsed = time.time() - start\n","        train_times.append(elapsed)\n","        print(f\"   Run {i+1}: {elapsed:.3f}s (loss: {loss.item():.4f})\")\n","\n","    avg_train = np.mean(train_times)\n","    std_train = np.std(train_times)\n","\n","    # --------------------------------------------------------\n","    # 4. RESULTS SUMMARY\n","    # --------------------------------------------------------\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"RESULTS\")\n","    print(\"=\"*70)\n","\n","    print(f\"\\nForward pass only:\")\n","    print(f\"  Average: {avg_forward:.3f}s ± {std_forward:.3f}s\")\n","\n","    print(f\"\\nForward + Backward (training):\")\n","    print(f\"  Average: {avg_train:.3f}s ± {std_train:.3f}s\")\n","\n","    print(f\"\\nBatch details:\")\n","    print(f\"  Batch size: {batch_targets['qpd'].shape[0]}\")\n","    print(f\"  Samples per batch: {batch_targets['qpd'].shape[0]}\")\n","    print(f\"  Using bfloat16: {USE_BF16}\")\n","    print(f\"  Using AMP: {USE_AMP}\")\n","\n","    # --------------------------------------------------------\n","    # 5. TIME PROJECTIONS\n","    # --------------------------------------------------------\n","    batches_per_epoch = len(train_loader)\n","    epoch_time_sec = avg_train * batches_per_epoch\n","    epoch_time_min = epoch_time_sec / 60\n","    total_time_min = epoch_time_min * NUM_EPOCHS\n","    total_time_hr = total_time_min / 60\n","\n","    print(f\"\\n\" + \"=\"*70)\n","    print(\"TIME PROJECTIONS\")\n","    print(\"=\"*70)\n","    print(f\"\\nConfiguration:\")\n","    print(f\"  Training samples: {len(train_dataset)}\")\n","    print(f\"  Batch size: {batch_targets['qpd'].shape[0]}\")\n","    print(f\"  Batches per epoch: {batches_per_epoch}\")\n","    print(f\"  Number of epochs: {NUM_EPOCHS}\")\n","\n","    print(f\"\\nEstimated times:\")\n","    print(f\"  Per epoch: {epoch_time_min:.1f} minutes ({epoch_time_sec:.0f} seconds)\")\n","    print(f\"  Total ({NUM_EPOCHS} epochs): {total_time_min:.1f} minutes ({total_time_hr:.2f} hours)\")\n","\n","    # Add validation time estimate (roughly 25% of training time)\n","    val_time_min = (total_time_min * 0.25)\n","    total_with_val = total_time_min + val_time_min\n","\n","    print(f\"  + Validation time: ~{val_time_min:.1f} minutes\")\n","    print(f\"  = Total training time: ~{total_with_val:.1f} minutes ({total_with_val/60:.2f} hours)\")\n","\n","    # --------------------------------------------------------\n","    # 6. PERFORMANCE ASSESSMENT\n","    # --------------------------------------------------------\n","    print(f\"\\n\" + \"=\"*70)\n","    print(\"ASSESSMENT\")\n","    print(\"=\"*70)\n","\n","    print(f\"  Expected total time: {total_with_val:.0f} minutes ({total_with_val/60:.1f} hours)\")\n","\n","    # --------------------------------------------------------\n","    # 7. GPU MEMORY USAGE\n","    # --------------------------------------------------------\n","    print(f\"\\n\" + \"=\"*70)\n","    print(\"GPU MEMORY\")\n","    print(\"=\"*70)\n","\n","    if torch.cuda.is_available():\n","        allocated = torch.cuda.memory_allocated() / 1e9\n","        reserved = torch.cuda.memory_reserved() / 1e9\n","        max_allocated = torch.cuda.max_memory_allocated() / 1e9\n","\n","        print(f\"\\nCurrent usage:\")\n","        print(f\"  Allocated: {allocated:.2f} GB\")\n","        print(f\"  Reserved: {reserved:.2f} GB\")\n","        print(f\"  Peak: {max_allocated:.2f} GB\")\n","\n","        # A100 has 40GB or 80GB\n","        gpu_name = torch.cuda.get_device_name(0)\n","        if '80GB' in gpu_name:\n","            total_memory = 80\n","        else:\n","            total_memory = 40\n","\n","        usage_pct = (max_allocated / total_memory) * 100\n","        print(f\"  Total GPU memory: {total_memory} GB\")\n","        print(f\"  Usage: {usage_pct:.1f}%\")\n","\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    print(f\"\\n\" + \"=\"*70)\n","    print(\"Benchmark complete! Ready to start training.\")\n","    print(\"=\"*70)\n","\n","    # Clean up\n","    optimizer.zero_grad()\n","    del batch_graphs, batch_targets, preds, loss\n","    torch.cuda.empty_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9EenfpvYFPq","executionInfo":{"status":"ok","timestamp":1765507020694,"user_tz":480,"elapsed":13778,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"bd898b88-1426-4a00-9f5d-85ce494719ec"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","PERFORMANCE BENCHMARK - Testing on A100 GPU\n","======================================================================\n","\n","1. Warming up GPU (JIT compilation, caching)...\n","Warmup complete\n","\n","2. Testing forward pass (inference)...\n","   Run 1: 0.535s\n","   Run 2: 0.567s\n","   Run 3: 0.532s\n","   Run 4: 0.534s\n","   Run 5: 0.544s\n","\n","3. Testing forward + backward pass (training)...\n","   Run 1: 1.063s (loss: 0.4515)\n","   Run 2: 1.049s (loss: 0.4512)\n","   Run 3: 1.052s (loss: 0.4510)\n","   Run 4: 1.060s (loss: 0.4509)\n","   Run 5: 1.053s (loss: 0.4512)\n","\n","======================================================================\n","RESULTS\n","======================================================================\n","\n","Forward pass only:\n","  Average: 0.542s ± 0.013s\n","\n","Forward + Backward (training):\n","  Average: 1.056s ± 0.005s\n","\n","Batch details:\n","  Batch size: 64\n","  Samples per batch: 64\n","  Using bfloat16: True\n","  Using AMP: True\n","\n","======================================================================\n","TIME PROJECTIONS\n","======================================================================\n","\n","Configuration:\n","  Training samples: 1834\n","  Batch size: 64\n","  Batches per epoch: 29\n","  Number of epochs: 50\n","\n","Estimated times:\n","  Per epoch: 0.5 minutes (31 seconds)\n","  Total (50 epochs): 25.5 minutes (0.43 hours)\n","  + Validation time: ~6.4 minutes\n","  = Total training time: ~31.9 minutes (0.53 hours)\n","\n","======================================================================\n","ASSESSMENT\n","======================================================================\n","  Expected total time: 32 minutes (0.5 hours)\n","\n","======================================================================\n","GPU MEMORY\n","======================================================================\n","\n","Current usage:\n","  Allocated: 0.10 GB\n","  Reserved: 2.92 GB\n","  Peak: 2.38 GB\n","  Total GPU memory: 40 GB\n","  Usage: 6.0%\n","\n","======================================================================\n","Benchmark complete! Ready to start training.\n","======================================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"training_loop_section"},"source":["## 8. Training Loop\n","\n","**Expected training time:** ~2 hours for 20 epochs with 2K samples\n","\n","**Target performance for milestone:**\n","- R² > 0.4 (shows model learns patterns)\n","- Loss decreasing steadily\n","- Better than simple baselines"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"training_loop","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765508566924,"user_tz":480,"elapsed":1546217,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"0a50df3f-46de-4462-e627-5462e21795e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","Starting Training\n","======================================================================\n","\n","Epoch 1/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:26<00:26,  1.78s/it, loss=1.4779]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 65.9%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:48<00:00,  1.66s/it, loss=0.5514]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.9582\n","  Val Loss:   0.8775\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.777     0.236      26.141    \n","    Answer Rate     0.142      -0.189     0.182     \n","    Retention       0.083      0.096      0.106     \n","    Mean            4.001      0.048      8.809     \n","    New best model saved (val_loss: 0.8775)\n","\n","Epoch 2/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:32,  1.16s/it, loss=0.4437]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 76.1%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  59%|█████▊    | 17/29 [00:21<00:14,  1.23s/it, loss=0.5386]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 80.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.7592]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.7948\n","  Val Loss:   0.7070\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.182     0.534      20.420    \n","    Answer Rate     0.131      -0.050     0.171     \n","    Retention       0.079      0.213      0.099     \n","    Mean            4.131      0.232      6.896     \n","    New best model saved (val_loss: 0.7070)\n","\n","Epoch 3/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  14%|█▍        | 4/29 [00:04<00:29,  1.16s/it, loss=0.6056]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 84.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:23<00:12,  1.28s/it, loss=0.7174]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 86.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.20s/it, loss=0.4337]\n"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 87.7%\n","\n","Results:\n","  Train Loss: 0.6502\n","  Val Loss:   0.7118\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             19.267     0.236      26.143    \n","    Answer Rate     0.127      0.012      0.166     \n","    Retention       0.072      0.327      0.091     \n","    Mean            6.489      0.192      8.800     \n","  No improvement (1/10)\n","\n","Epoch 4/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:06<00:25,  1.11s/it, loss=0.5066]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 88.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  76%|███████▌  | 22/29 [00:25<00:07,  1.14s/it, loss=0.4995]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 89.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4386]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5869\n","  Val Loss:   0.8708\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             20.488     -0.258     33.545    \n","    Answer Rate     0.124      0.041      0.163     \n","    Retention       0.072      0.284      0.094     \n","    Mean            6.895      0.023      11.267    \n","  No improvement (2/10)\n","\n","Epoch 5/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 9/29 [00:11<00:24,  1.22s/it, loss=0.3557]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 90.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:29<00:06,  1.24s/it, loss=0.4865]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 91.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.6305]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5881\n","  Val Loss:   0.6354\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.564     0.546      20.147    \n","    Answer Rate     0.126      0.012      0.166     \n","    Retention       0.070      0.339      0.090     \n","    Mean            3.920      0.299      6.801     \n","    New best model saved (val_loss: 0.6354)\n","\n","  Dataset Cache: 13965 items | Hit rate: 91.8%\n","\n","Epoch 6/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 11/29 [00:13<00:21,  1.17s/it, loss=0.3792]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 92.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  93%|█████████▎| 27/29 [00:31<00:02,  1.21s/it, loss=0.8106]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 92.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.8601]\n","Evaluating:  67%|██████▋   | 4/6 [00:02<00:01,  1.77it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 92.9%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5825\n","  Val Loss:   0.6506\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             15.756     0.397      23.225    \n","    Answer Rate     0.128      -0.008     0.167     \n","    Retention       0.069      0.372      0.088     \n","    Mean            5.317      0.254      7.827     \n","  No improvement (1/10)\n","\n","Epoch 7/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:17<00:18,  1.24s/it, loss=0.6016]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 93.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.4815]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5424\n","  Val Loss:   0.6846\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             15.802     0.229      26.269    \n","    Answer Rate     0.130      -0.038     0.170     \n","    Retention       0.069      0.379      0.088     \n","    Mean            5.334      0.190      8.842     \n","  No improvement (2/10)\n","\n","Epoch 8/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:29,  1.07s/it, loss=0.4103]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 93.9%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  59%|█████▊    | 17/29 [00:20<00:15,  1.26s/it, loss=0.5058]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 94.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.4461]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5224\n","  Val Loss:   0.6917\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             16.466     0.236      26.148    \n","    Answer Rate     0.132      -0.067     0.172     \n","    Retention       0.070      0.363      0.089     \n","    Mean            5.556      0.177      8.803     \n","  No improvement (3/10)\n","\n","Epoch 9/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  14%|█▍        | 4/29 [00:04<00:29,  1.19s/it, loss=0.4105]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 94.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:22<00:11,  1.20s/it, loss=0.4958]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 94.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.3142]\n","Evaluating:  67%|██████▋   | 4/6 [00:02<00:01,  1.74it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 95.1%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5198\n","  Val Loss:   0.7216\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             17.782     0.058      29.027    \n","    Answer Rate     0.125      0.022      0.165     \n","    Retention       0.067      0.381      0.087     \n","    Mean            5.991      0.154      9.760     \n","  No improvement (4/10)\n","\n","Epoch 10/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:06<00:25,  1.10s/it, loss=0.4714]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 95.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  76%|███████▌  | 22/29 [00:25<00:07,  1.09s/it, loss=0.3947]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 95.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.5244]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5199\n","  Val Loss:   0.6439\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             14.745     0.302      24.993    \n","    Answer Rate     0.122      0.062      0.162     \n","    Retention       0.068      0.389      0.087     \n","    Mean            4.978      0.251      8.414     \n","  No improvement (5/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 95.5%\n","\n","Epoch 11/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 9/29 [00:11<00:24,  1.20s/it, loss=0.4738]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 95.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:29<00:05,  1.19s/it, loss=0.4441]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 95.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.4135]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5134\n","  Val Loss:   0.5674\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             13.150     0.631      18.161    \n","    Answer Rate     0.123      0.048      0.163     \n","    Retention       0.066      0.399      0.086     \n","    Mean            4.447      0.360      6.137     \n","    New best model saved (val_loss: 0.5674)\n","\n","Epoch 12/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 11/29 [00:13<00:23,  1.32s/it, loss=0.7833]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.0%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  93%|█████████▎| 27/29 [00:32<00:02,  1.18s/it, loss=0.4092]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.1%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:33<00:00,  1.17s/it, loss=0.6913]\n","Evaluating:  67%|██████▋   | 4/6 [00:02<00:01,  1.70it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.2%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5024\n","  Val Loss:   0.6589\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             16.222     0.410      22.975    \n","    Answer Rate     0.124      0.038      0.164     \n","    Retention       0.074      0.313      0.092     \n","    Mean            5.473      0.254      7.744     \n","  No improvement (1/10)\n","\n","Epoch 13/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:17<00:18,  1.25s/it, loss=0.5724]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4037]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5206\n","  Val Loss:   0.5734\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.780     0.564      19.755    \n","    Answer Rate     0.118      0.111      0.157     \n","    Retention       0.065      0.402      0.086     \n","    Mean            4.321      0.359      6.666     \n","  No improvement (2/10)\n","\n","Epoch 14/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:33,  1.20s/it, loss=0.4099]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  59%|█████▊    | 17/29 [00:20<00:15,  1.27s/it, loss=0.9542]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.6895]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.5049\n","  Val Loss:   0.6120\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.696     0.402      23.136    \n","    Answer Rate     0.118      0.114      0.157     \n","    Retention       0.064      0.401      0.086     \n","    Mean            3.959      0.306      7.793     \n","  No improvement (3/10)\n","\n","Epoch 15/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  14%|█▍        | 4/29 [00:04<00:32,  1.30s/it, loss=0.4028]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:23<00:12,  1.25s/it, loss=0.7735]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.4426]\n","Evaluating:  67%|██████▋   | 4/6 [00:02<00:01,  1.78it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.9%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4934\n","  Val Loss:   0.6178\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.075     0.518      20.762    \n","    Answer Rate     0.120      0.086      0.159     \n","    Retention       0.066      0.361      0.089     \n","    Mean            3.754      0.322      7.003     \n","  No improvement (4/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 96.9%\n","\n","Epoch 16/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:06<00:27,  1.20s/it, loss=0.3889]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 96.9%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  76%|███████▌  | 22/29 [00:25<00:08,  1.20s/it, loss=0.3183]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.0%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.8286]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4718\n","  Val Loss:   0.6972\n","  LR: 0.001000\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             15.646     -0.036     30.438    \n","    Answer Rate     0.118      0.113      0.157     \n","    Retention       0.063      0.434      0.084     \n","    Mean            5.276      0.170      10.226    \n","  No improvement (5/10)\n","\n","Epoch 17/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 9/29 [00:10<00:23,  1.17s/it, loss=0.4189]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.1%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:28<00:05,  1.18s/it, loss=0.4705]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.5395]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4733\n","  Val Loss:   0.6167\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.218     0.586      19.255    \n","    Answer Rate     0.121      0.064      0.161     \n","    Retention       0.067      0.334      0.091     \n","    Mean            4.136      0.328      6.502     \n","  No improvement (6/10)\n","\n","Epoch 18/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 11/29 [00:12<00:21,  1.21s/it, loss=0.3497]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  93%|█████████▎| 27/29 [00:31<00:02,  1.20s/it, loss=0.8026]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.7646]\n","Evaluating:  50%|█████     | 3/6 [00:01<00:01,  1.85it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.4%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4735\n","  Val Loss:   0.6166\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.460     0.280      25.381    \n","    Answer Rate     0.118      0.117      0.157     \n","    Retention       0.062      0.433      0.084     \n","    Mean            3.880      0.277      8.540     \n","  No improvement (7/10)\n","\n","Epoch 19/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:16<00:18,  1.24s/it, loss=0.5375]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.3880]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4531\n","  Val Loss:   0.5200\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.296     0.665      17.311    \n","    Answer Rate     0.118      0.121      0.156     \n","    Retention       0.063      0.434      0.084     \n","    Mean            3.492      0.407      5.850     \n","    New best model saved (val_loss: 0.5200)\n","\n","Epoch 20/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:38,  1.37s/it, loss=0.4707]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▌    | 16/29 [00:19<00:14,  1.12s/it, loss=0.3789]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.3660]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4691\n","  Val Loss:   0.6373\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             15.578     0.222      26.383    \n","    Answer Rate     0.117      0.130      0.156     \n","    Retention       0.062      0.426      0.084     \n","    Mean            5.253      0.260      8.874     \n","  No improvement (1/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 97.6%\n","\n","Epoch 21/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 3/29 [00:03<00:28,  1.09s/it, loss=0.4132]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:23<00:13,  1.36s/it, loss=0.9057]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4336]\n","Evaluating:  50%|█████     | 3/6 [00:01<00:01,  1.83it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.7%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4632\n","  Val Loss:   0.6369\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.722     0.462      21.942    \n","    Answer Rate     0.116      0.117      0.157     \n","    Retention       0.068      0.327      0.091     \n","    Mean            3.635      0.302      7.397     \n","  No improvement (2/10)\n","\n","Epoch 22/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:06<00:25,  1.12s/it, loss=0.4014]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  76%|███████▌  | 22/29 [00:25<00:08,  1.16s/it, loss=0.5090]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=1.0870]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4795\n","  Val Loss:   0.6717\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             15.954     0.228      26.282    \n","    Answer Rate     0.117      0.130      0.156     \n","    Retention       0.065      0.373      0.088     \n","    Mean            5.379      0.244      8.842     \n","  No improvement (3/10)\n","\n","Epoch 23/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 9/29 [00:10<00:23,  1.15s/it, loss=0.4386]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.9%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:28<00:06,  1.21s/it, loss=0.3989]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 97.9%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.6307]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4535\n","  Val Loss:   0.5326\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             9.580      0.609      18.695    \n","    Answer Rate     0.118      0.123      0.156     \n","    Retention       0.062      0.434      0.084     \n","    Mean            3.253      0.389      6.312     \n","  No improvement (4/10)\n","\n","Epoch 24/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 11/29 [00:13<00:20,  1.13s/it, loss=0.3330]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.0%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  93%|█████████▎| 27/29 [00:32<00:02,  1.22s/it, loss=0.4611]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.0%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4970]\n","Evaluating:  50%|█████     | 3/6 [00:01<00:01,  1.83it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.0%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4516\n","  Val Loss:   0.5943\n","  LR: 0.000500\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.894     0.581      19.352    \n","    Answer Rate     0.115      0.137      0.155     \n","    Retention       0.066      0.354      0.089     \n","    Mean            4.358      0.358      6.532     \n","  No improvement (5/10)\n","\n","Epoch 25/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:17<00:18,  1.25s/it, loss=0.3958]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.1%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.4178]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4421\n","  Val Loss:   0.5799\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.320     0.421      22.765    \n","    Answer Rate     0.114      0.145      0.154     \n","    Retention       0.064      0.421      0.085     \n","    Mean            3.833      0.329      7.668     \n","  No improvement (6/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 98.1%\n","\n","Epoch 26/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:30,  1.09s/it, loss=0.4207]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.1%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▌    | 16/29 [00:19<00:15,  1.23s/it, loss=0.5341]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.1%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.20s/it, loss=0.3680]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4306\n","  Val Loss:   0.5302\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.399     0.624      18.347    \n","    Answer Rate     0.115      0.134      0.155     \n","    Retention       0.062      0.435      0.084     \n","    Mean            3.525      0.398      6.195     \n","  No improvement (7/10)\n","\n","Epoch 27/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 3/29 [00:03<00:33,  1.29s/it, loss=0.2990]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:23<00:12,  1.26s/it, loss=0.4556]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4422]\n","Evaluating:  50%|█████     | 3/6 [00:01<00:01,  1.83it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.2%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4305\n","  Val Loss:   0.5178\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             9.830      0.667      17.270    \n","    Answer Rate     0.116      0.135      0.155     \n","    Retention       0.064      0.424      0.084     \n","    Mean            3.337      0.409      5.836     \n","    New best model saved (val_loss: 0.5178)\n","\n","Epoch 28/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:07<00:29,  1.29s/it, loss=0.4408]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.2%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  76%|███████▌  | 22/29 [00:26<00:08,  1.18s/it, loss=0.3045]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4544]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4296\n","  Val Loss:   0.5768\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.075     0.546      20.161    \n","    Answer Rate     0.114      0.138      0.155     \n","    Retention       0.064      0.390      0.087     \n","    Mean            4.085      0.358      6.801     \n","  No improvement (1/10)\n","\n","Epoch 29/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 9/29 [00:11<00:25,  1.25s/it, loss=0.5028]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:29<00:05,  1.16s/it, loss=0.3372]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.3%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.5254]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4311\n","  Val Loss:   0.5219\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.376     0.600      18.918    \n","    Answer Rate     0.114      0.143      0.154     \n","    Retention       0.062      0.446      0.083     \n","    Mean            3.517      0.397      6.385     \n","  No improvement (2/10)\n","\n","Epoch 30/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 11/29 [00:13<00:22,  1.24s/it, loss=0.3753]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  93%|█████████▎| 27/29 [00:32<00:02,  1.27s/it, loss=0.5349]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.3961]\n","Evaluating:  50%|█████     | 3/6 [00:01<00:01,  1.88it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.4%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4317\n","  Val Loss:   0.5576\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.475     0.522      20.669    \n","    Answer Rate     0.115      0.146      0.154     \n","    Retention       0.065      0.412      0.085     \n","    Mean            3.885      0.360      6.970     \n","  No improvement (3/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 98.4%\n","\n","Epoch 31/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:17<00:17,  1.18s/it, loss=0.2882]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.4%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.5561]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4281\n","  Val Loss:   0.5103\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             9.520      0.682      16.858    \n","    Answer Rate     0.113      0.154      0.153     \n","    Retention       0.062      0.431      0.084     \n","    Mean            3.232      0.422      5.699     \n","    New best model saved (val_loss: 0.5103)\n","\n","Epoch 32/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:37,  1.35s/it, loss=0.3734]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▌    | 16/29 [00:18<00:13,  1.06s/it, loss=0.3085]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=1.3231]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4494\n","  Val Loss:   0.5399\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.871     0.556      19.938    \n","    Answer Rate     0.118      0.120      0.156     \n","    Retention       0.062      0.443      0.083     \n","    Mean            4.350      0.373      6.726     \n","  No improvement (1/10)\n","\n","Epoch 33/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 3/29 [00:03<00:28,  1.08s/it, loss=0.4047]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:23<00:11,  1.14s/it, loss=0.4245]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.5%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.4458]\n","Evaluating:  33%|███▎      | 2/6 [00:01<00:02,  1.64it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.5%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4292\n","  Val Loss:   0.5405\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.491     0.529      20.529    \n","    Answer Rate     0.115      0.144      0.154     \n","    Retention       0.061      0.450      0.082     \n","    Mean            3.889      0.374      6.922     \n","  No improvement (2/10)\n","\n","Epoch 34/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:07<00:28,  1.23s/it, loss=0.3758]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  76%|███████▌  | 22/29 [00:26<00:10,  1.46s/it, loss=0.4185]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.20s/it, loss=0.4784]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4206\n","  Val Loss:   0.5737\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             13.029     0.482      21.525    \n","    Answer Rate     0.114      0.139      0.155     \n","    Retention       0.062      0.421      0.085     \n","    Mean            4.402      0.347      7.255     \n","  No improvement (3/10)\n","\n","Epoch 35/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  31%|███       | 9/29 [00:10<00:22,  1.14s/it, loss=0.4633]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:29<00:06,  1.25s/it, loss=0.6598]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.5219]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4131\n","  Val Loss:   0.5388\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.782     0.580      19.374    \n","    Answer Rate     0.114      0.135      0.155     \n","    Retention       0.061      0.435      0.084     \n","    Mean            3.652      0.383      6.538     \n","  No improvement (4/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 98.6%\n","\n","Epoch 36/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  38%|███▊      | 11/29 [00:12<00:21,  1.17s/it, loss=0.3265]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.6%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  93%|█████████▎| 27/29 [00:32<00:02,  1.21s/it, loss=0.5890]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.9007]\n","Evaluating:  33%|███▎      | 2/6 [00:01<00:02,  1.65it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.7%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4189\n","  Val Loss:   0.5355\n","  LR: 0.000250\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             12.025     0.564      19.742    \n","    Answer Rate     0.114      0.155      0.153     \n","    Retention       0.062      0.436      0.084     \n","    Mean            4.067      0.385      6.660     \n","  No improvement (5/10)\n","\n","Epoch 37/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  48%|████▊     | 14/29 [00:16<00:19,  1.27s/it, loss=0.4348]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.3465]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4272\n","  Val Loss:   0.5423\n","  LR: 0.000125\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             11.909     0.521      20.692    \n","    Answer Rate     0.113      0.157      0.153     \n","    Retention       0.061      0.445      0.083     \n","    Mean            4.028      0.374      6.976     \n","  No improvement (6/10)\n","\n","Epoch 38/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   3%|▎         | 1/29 [00:01<00:33,  1.21s/it, loss=0.4409]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  55%|█████▌    | 16/29 [00:19<00:15,  1.20s/it, loss=0.2521]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.4266]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4079\n","  Val Loss:   0.5199\n","  LR: 0.000125\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.199     0.606      18.783    \n","    Answer Rate     0.114      0.147      0.154     \n","    Retention       0.061      0.447      0.083     \n","    Mean            3.458      0.400      6.340     \n","  No improvement (7/10)\n","\n","Epoch 39/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  10%|█         | 3/29 [00:03<00:28,  1.10s/it, loss=0.4086]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.7%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  66%|██████▌   | 19/29 [00:22<00:11,  1.18s/it, loss=0.3082]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.19s/it, loss=0.6686]\n","Evaluating:  33%|███▎      | 2/6 [00:01<00:02,  1.62it/s]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.8%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4100\n","  Val Loss:   0.5176\n","  LR: 0.000125\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.270     0.616      18.531    \n","    Answer Rate     0.114      0.152      0.154     \n","    Retention       0.061      0.447      0.083     \n","    Mean            3.482      0.405      6.256     \n","  No improvement (8/10)\n","\n","Epoch 40/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  21%|██        | 6/29 [00:06<00:25,  1.10s/it, loss=0.4561]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  72%|███████▏  | 21/29 [00:24<00:08,  1.09s/it, loss=0.4343]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.18s/it, loss=0.5025]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4165\n","  Val Loss:   0.5242\n","  LR: 0.000125\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.515     0.612      18.636    \n","    Answer Rate     0.113      0.151      0.154     \n","    Retention       0.061      0.438      0.083     \n","    Mean            3.563      0.400      6.291     \n","  No improvement (9/10)\n","\n","  Dataset Cache: 13965 items | Hit rate: 98.8%\n","\n","Epoch 41/50\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:  28%|██▊       | 8/29 [00:09<00:24,  1.18s/it, loss=0.2683]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training:  83%|████████▎ | 24/29 [00:28<00:05,  1.18s/it, loss=0.5187]"]},{"output_type":"stream","name":"stdout","text":["  Cache: 13965/None graphs | Hit rate: 98.8%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 29/29 [00:34<00:00,  1.17s/it, loss=0.4420]\n","                                                         "]},{"output_type":"stream","name":"stdout","text":["\n","Results:\n","  Train Loss: 0.4115\n","  Val Loss:   0.5247\n","  LR: 0.000125\n","\n","  Validation Metrics:\n","    Metric          MAE        R²         RMSE      \n","    ---------------------------------------------\n","    Qpd             10.610     0.608      18.736    \n","    Answer Rate     0.114      0.145      0.154     \n","    Retention       0.061      0.442      0.083     \n","    Mean            3.595      0.398      6.324     \n","  No improvement (10/10)\n","\n","Early stopping triggered after 41 epochs\n","\n","======================================================================\n","Training Complete!\n","======================================================================\n","Best validation loss: 0.5103\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["# Training loop with early stopping - TRACKS ALL METRICS\n","best_val_loss = float('inf')\n","patience_counter = 0\n","train_losses = []\n","val_losses = []\n","\n","# Track all metrics for each target variable\n","val_metrics_history = {\n","    'qpd': {'mae': [], 'r2': [], 'rmse': []},\n","    'answer_rate': {'mae': [], 'r2': [], 'rmse': []},\n","    'retention': {'mae': [], 'r2': [], 'rmse': []}\n","}\n","\n","print(\"=\"*70)\n","print(\"Starting Training\")\n","print(\"=\"*70)\n","\n","for epoch in range(NUM_EPOCHS):\n","    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n","    print(\"-\" * 70)\n","\n","    # Train\n","    train_loss = train_epoch(\n","        model, train_loader, optimizer, criterion,\n","        device, scaler, USE_AMP, scheduler=None\n","    )\n","    train_losses.append(train_loss)\n","\n","    # Use the training dataset's normalization stats for evaluation\n","    norm_stats = train_dataset.norm_stats\n","\n","    # Validate - returns metrics dict now\n","    val_loss, metrics = evaluate(\n","        model, val_loader, criterion, device, USE_AMP, norm_stats\n","    )\n","    val_losses.append(val_loss)\n","\n","    # Store all metrics\n","    for key in metrics:\n","        for metric_name in ['mae', 'r2', 'rmse']:\n","            val_metrics_history[key][metric_name].append(metrics[key][metric_name])\n","\n","    # Learning rate step\n","    scheduler.step(val_loss)\n","\n","    # Print results\n","    print(f\"\\nResults:\")\n","    print(f\"  Train Loss: {train_loss:.4f}\")\n","    print(f\"  Val Loss:   {val_loss:.4f}\")\n","    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","    print(f\"\\n  Validation Metrics:\")\n","    print(f\"    {'Metric':<15} {'MAE':<10} {'R²':<10} {'RMSE':<10}\")\n","    print(f\"    {'-'*45}\")\n","    for key in ['qpd', 'answer_rate', 'retention']:\n","        if key in metrics:\n","            print(f\"    {key.replace('_', ' ').title():<15} \"\n","                  f\"{metrics[key]['mae']:<10.3f} \"\n","                  f\"{metrics[key]['r2']:<10.3f} \"\n","                  f\"{metrics[key]['rmse']:<10.3f}\")\n","\n","    # Compute mean metrics across all targets\n","    mean_mae = np.mean([metrics[k]['mae'] for k in metrics])\n","    mean_r2 = np.mean([metrics[k]['r2'] for k in metrics])\n","    mean_rmse = np.mean([metrics[k]['rmse'] for k in metrics])\n","    print(f\"    {'Mean':<15} {mean_mae:<10.3f} {mean_r2:<10.3f} {mean_rmse:<10.3f}\")\n","\n","    # Early stopping check (based on validation loss)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","\n","        # Save best model\n","        if IN_COLAB:\n","            model_path = f\"{PROJECT_DIR}/results/baseline_gnn/best_model.pt\"\n","        else:\n","            model_path = \"../results/baseline_gnn/best_model.pt\"\n","\n","        # Create results directory if it doesn't exist\n","        import os\n","        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n","\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_loss': val_loss,\n","            'metrics': metrics,\n","            'train_losses': train_losses,\n","            'val_losses': val_losses,\n","            'val_metrics_history': val_metrics_history\n","        }, model_path)\n","\n","        print(f\"    New best model saved (val_loss: {val_loss:.4f})\")\n","    else:\n","        patience_counter += 1\n","        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n","\n","        if patience_counter >= PATIENCE:\n","            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n","            break\n","\n","    # Show cache statistics every 5 epochs\n","    if (epoch + 1) % 5 == 0:\n","        cache_info = train_dataset.get_cache_stats()\n","        total = cache_info.hits + cache_info.misses\n","        hit_rate = cache_info.hits / total if total > 0 else 0.0\n","        print(f\"\\n  Dataset Cache: {cache_info.currsize} items | Hit rate: {hit_rate:.1%}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"Training Complete!\")\n","print(\"=\"*70)\n","print(f\"Best validation loss: {best_val_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"results_section"},"source":["## 9. Visualize Results"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"plot_results","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1YbmOWv_Vsy2U6wyLgHnKOgV7Ssa5-mfL"},"executionInfo":{"status":"ok","timestamp":1765508579616,"user_tz":480,"elapsed":12660,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}},"outputId":"0af94aed-f312-4805-87d3-2fa7e780ee2e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import os\n","\n","# Set style\n","sns.set_style(\"whitegrid\")\n","sns.set_context(\"paper\", font_scale=1.2)\n","plt.rcParams['figure.dpi'] = 500\n","plt.rcParams['savefig.dpi'] = 500\n","plt.rcParams['font.family'] = 'sans-serif'\n","\n","# Create results directory\n","results_dir = f\"{PROJECT_DIR}/results/baseline_gnn\" if IN_COLAB else \"../results/baseline_gnn\"\n","os.makedirs(results_dir, exist_ok=True)\n","\n","# Color palette\n","colors = sns.color_palette(\"husl\", 3)\n","metric_colors = {'qpd': '#2E86AB', 'answer_rate': '#A23B72', 'retention': '#F18F01'}\n","\n","# ============================================================\n","# Figure 1: Training & Validation Loss\n","# ============================================================\n","fig, ax = plt.subplots(figsize=(8, 5))\n","\n","epochs = np.arange(1, len(train_losses) + 1)\n","ax.plot(epochs, train_losses, marker='o', linewidth=2.5,\n","        label='Training Loss', color=colors[0], markersize=6)\n","ax.plot(epochs, val_losses, marker='s', linewidth=2.5,\n","        label='Validation Loss', color=colors[1], markersize=6)\n","\n","ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n","ax.set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n","ax.set_title('Training Progress', fontsize=14, fontweight='bold', pad=15)\n","ax.legend(frameon=True, loc='best', fontsize=11)\n","ax.grid(True, alpha=0.3, linestyle='--')\n","\n","plt.tight_layout()\n","plt.savefig(f\"{results_dir}/loss_curves.png\", bbox_inches='tight')\n","print(f\"Saved: {results_dir}/loss_curves.png\")\n","plt.show()\n","\n","# ============================================================\n","# Figure 2: R² Performance Over Time\n","# ============================================================\n","fig, ax = plt.subplots(figsize=(8, 5))\n","\n","targets = ['qpd', 'answer_rate', 'retention']\n","labels = {'qpd': 'Questions Per Day', 'answer_rate': 'Answer Rate', 'retention': 'User Retention'}\n","\n","for target in targets:\n","    r2_values = val_metrics_history[target]['r2']\n","    ax.plot(epochs, r2_values, marker='o', linewidth=2.5,\n","            label=labels[target], color=metric_colors[target], markersize=6)\n","\n","ax.axhline(y=0.4, color='gray', linestyle='--', alpha=0.5, linewidth=2, label='Target (0.4)')\n","ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n","ax.set_ylabel('R² Score', fontsize=12, fontweight='bold')\n","ax.set_title('R² Performance by Metric', fontsize=14, fontweight='bold', pad=15)\n","ax.legend(frameon=True, loc='best', fontsize=10)\n","ax.grid(True, alpha=0.3, linestyle='--')\n","ax.set_ylim(bottom=0)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{results_dir}/r2_over_time.png\", bbox_inches='tight')\n","print(f\"Saved: {results_dir}/r2_over_time.png\")\n","plt.show()\n","\n","# ============================================================\n","# Figure 3: Final Performance Comparison\n","# ============================================================\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# Prepare data\n","metric_names = [labels[t] for t in targets]\n","final_r2 = [val_metrics_history[t]['r2'][-1] for t in targets]\n","final_mae = [val_metrics_history[t]['mae'][-1] for t in targets]\n","\n","# R² comparison\n","ax1 = axes[0]\n","bars = ax1.barh(metric_names, final_r2, color=[metric_colors[t] for t in targets], alpha=0.8)\n","ax1.axvline(x=0.4, color='gray', linestyle='--', linewidth=2, alpha=0.5, label='Target')\n","ax1.set_xlabel('R² Score', fontsize=12, fontweight='bold')\n","ax1.set_title('R² Performance', fontsize=13, fontweight='bold', pad=10)\n","ax1.set_xlim(0, max(final_r2) * 1.2)\n","ax1.grid(True, alpha=0.3, linestyle='--', axis='x')\n","\n","# Add value labels\n","for i, (bar, val) in enumerate(zip(bars, final_r2)):\n","    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n","             f'{val:.3f}', va='center', fontsize=11, fontweight='bold')\n","\n","# MAE comparison\n","ax2 = axes[1]\n","bars = ax2.barh(metric_names, final_mae, color=[metric_colors[t] for t in targets], alpha=0.8)\n","ax2.set_xlabel('Mean Absolute Error', fontsize=12, fontweight='bold')\n","ax2.set_title('MAE Performance', fontsize=13, fontweight='bold', pad=10)\n","ax2.grid(True, alpha=0.3, linestyle='--', axis='x')\n","ax2.invert_xaxis()  # Lower is better\n","\n","# Add value labels\n","for i, (bar, val) in enumerate(zip(bars, final_mae)):\n","    ax2.text(val - 0.01, bar.get_y() + bar.get_height()/2,\n","             f'{val:.3f}', va='center', ha='right', fontsize=11, fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig(f\"{results_dir}/final_performance.png\", bbox_inches='tight')\n","print(f\"Saved: {results_dir}/final_performance.png\")\n","plt.show()\n","\n","# ============================================================\n","# Figure 4: Detailed Metric Evolution (3 subplots)\n","# ============================================================\n","fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","for idx, (target, ax) in enumerate(zip(targets, axes)):\n","    # Plot MAE and RMSE\n","    mae_values = val_metrics_history[target]['mae']\n","    rmse_values = val_metrics_history[target]['rmse']\n","    r2_values = val_metrics_history[target]['r2']\n","\n","    color = metric_colors[target]\n","\n","    # Primary axis: MAE and RMSE\n","    ax.plot(epochs, mae_values, marker='o', linewidth=2,\n","            label='MAE', color=color, markersize=5)\n","    ax.plot(epochs, rmse_values, marker='s', linewidth=2,\n","            label='RMSE', color=color, alpha=0.6, markersize=5, linestyle='--')\n","\n","    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n","    ax.set_ylabel('Error', fontsize=11, fontweight='bold')\n","    ax.set_title(labels[target], fontsize=12, fontweight='bold', pad=10)\n","    ax.grid(True, alpha=0.3, linestyle='--')\n","    ax.legend(loc='upper left', fontsize=9)\n","\n","    # Secondary axis: R²\n","    ax2 = ax.twinx()\n","    ax2.plot(epochs, r2_values, marker='^', linewidth=2,\n","             label='R²', color='green', alpha=0.7, markersize=5)\n","    ax2.axhline(y=0.4, color='gray', linestyle=':', alpha=0.5)\n","    ax2.set_ylabel('R²', fontsize=11, fontweight='bold', color='green')\n","    ax2.tick_params(axis='y', labelcolor='green')\n","    ax2.legend(loc='upper right', fontsize=9)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{results_dir}/metric_evolution.png\", bbox_inches='tight')\n","print(f\"Saved: {results_dir}/metric_evolution.png\")\n","plt.show()\n","\n","# ============================================================\n","# Figure 5: Results Summary Table (as image)\n","# ============================================================\n","fig, ax = plt.subplots(figsize=(10, 4))\n","ax.axis('tight')\n","ax.axis('off')\n","\n","# Prepare table data\n","table_data = []\n","table_data.append(['Metric', 'MAE ↓', 'RMSE ↓', 'R² ↑', 'Normalized MAE'])\n","\n","norm_stats = train_dataset.norm_stats\n","for target in targets:\n","    final_mae = val_metrics_history[target]['mae'][-1]\n","    final_rmse = val_metrics_history[target]['rmse'][-1]\n","    final_r2 = val_metrics_history[target]['r2'][-1]\n","    norm_mae = final_mae / norm_stats[target]['std']\n","\n","    table_data.append([\n","        labels[target],\n","        f'{final_mae:.4f}',\n","        f'{final_rmse:.4f}',\n","        f'{final_r2:.4f}',\n","        f'{norm_mae:.4f}'\n","    ])\n","\n","# Add mean row\n","mean_mae = np.mean([val_metrics_history[t]['mae'][-1] for t in targets])\n","mean_rmse = np.mean([val_metrics_history[t]['rmse'][-1] for t in targets])\n","mean_r2 = np.mean([val_metrics_history[t]['r2'][-1] for t in targets])\n","mean_norm_mae = np.mean([val_metrics_history[t]['mae'][-1] / norm_stats[t]['std'] for t in targets])\n","\n","table_data.append([\n","    'MEAN',\n","    f'{mean_mae:.4f}',\n","    f'{mean_rmse:.4f}',\n","    f'{mean_r2:.4f}',\n","    f'{mean_norm_mae:.4f}'\n","])\n","\n","# Create table\n","table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n","                colWidths=[0.25, 0.15, 0.15, 0.15, 0.2])\n","table.auto_set_font_size(False)\n","table.set_fontsize(11)\n","table.scale(1, 2)\n","\n","# Style header row\n","for i in range(5):\n","    cell = table[(0, i)]\n","    cell.set_facecolor('#2E86AB')\n","    cell.set_text_props(weight='bold', color='white')\n","\n","# Style mean row\n","for i in range(5):\n","    cell = table[(len(table_data)-1, i)]\n","    cell.set_facecolor('#E8E8E8')\n","    cell.set_text_props(weight='bold')\n","\n","# Alternate row colors\n","for i in range(1, len(table_data)-1):\n","    for j in range(5):\n","        if i % 2 == 0:\n","            table[(i, j)].set_facecolor('#F5F5F5')\n","\n","plt.title('Final Validation Results', fontsize=14, fontweight='bold', pad=20)\n","plt.savefig(f\"{results_dir}/results_table.png\", bbox_inches='tight')\n","print(f\"Saved: {results_dir}/results_table.png\")\n","plt.show()\n","\n","# ============================================================\n","# Print Summary\n","# ============================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"VISUALIZATION SUMMARY\")\n","print(\"=\"*70)\n","print(f\"\\nAll figures saved to: {results_dir}/\")\n","print(\"\\nFiles created:\")\n","print(\"  1. loss_curves.png       - Training/validation loss\")\n","print(\"  2. r2_over_time.png      - R² evolution by metric\")\n","print(\"  3. final_performance.png - Final R² and MAE comparison\")\n","print(\"  4. metric_evolution.png  - Detailed metric tracking\")\n","print(\"  5. results_table.png     - Summary table\")\n","print(\"\\n\" + \"=\"*70)\n","\n","# ============================================================\n","# Detailed Text Summary\n","# ============================================================\n","print(\"\\nFINAL RESULTS SUMMARY\")\n","print(\"=\"*70)\n","print(f\"\\nBest Validation Loss: {best_val_loss:.4f}\")\n","print(f\"Achieved at Epoch: {checkpoint['epoch'] if 'checkpoint' in globals() else 'N/A'}\\n\")\n","\n","print(\"Per-Metric Performance:\")\n","print(f\"{'Metric':<20} {'MAE ↓':<12} {'RMSE ↓':<12} {'R² ↑':<12} {'Norm MAE ↓':<12}\")\n","print(\"-\" * 68)\n","\n","for target in targets:\n","    final_mae = val_metrics_history[target]['mae'][-1]\n","    final_rmse = val_metrics_history[target]['rmse'][-1]\n","    final_r2 = val_metrics_history[target]['r2'][-1]\n","    norm_mae = final_mae / norm_stats[target]['std']\n","\n","    print(f\"{labels[target]:<20} {final_mae:<12.4f} {final_rmse:<12.4f} \"\n","          f\"{final_r2:<12.4f} {norm_mae:<12.4f}\")\n","\n","print(\"-\" * 68)\n","print(f\"{'MEAN':<20} {mean_mae:<12.4f} {mean_rmse:<12.4f} \"\n","      f\"{mean_r2:<12.4f} {mean_norm_mae:<12.4f}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"INTERPRETATION\")\n","print(\"=\"*70)\n","\n","if mean_r2 > 0.5:\n","    print(\"✓ Strong predictive performance (R² > 0.5)\")\n","elif mean_r2 > 0.4:\n","    print(\"✓ Good predictive performance (R² > 0.4)\")\n","elif mean_r2 > 0.3:\n","    print(\"⚠ Moderate performance (R² > 0.3)\")\n","    print(\"  Consider: Temporal distribution shift (COVID-19 period in validation)\")\n","else:\n","    print(\"⚠ Weak performance (R² < 0.3)\")\n","\n","print(\"\\nPractical Interpretation:\")\n","print(f\"  • Questions/Day:  ±{val_metrics_history['qpd']['mae'][-1]:.2f} questions\")\n","print(f\"  • Answer Rate:    ±{val_metrics_history['answer_rate']['mae'][-1]:.1%}\")\n","print(f\"  • User Retention: ±{val_metrics_history['retention']['mae'][-1]:.1%}\")\n","\n","print(\"\\nNormalized MAE (lower is better):\")\n","for target in targets:\n","    norm_mae = val_metrics_history[target]['mae'][-1] / norm_stats[target]['std']\n","    if norm_mae < 0.5:\n","        status = \"✓ Excellent\"\n","    elif norm_mae < 1.0:\n","        status = \"✓ Good\"\n","    else:\n","        status = \"⚠ Needs improvement\"\n","    print(f\"  • {labels[target]:<17}: {norm_mae:.3f} {status}\")\n","\n","print(\"\\n\" + \"=\"*70)"]},{"cell_type":"markdown","source":["## 10. Save GNN Results"],"metadata":{"id":"zUHFGffhZo56"}},{"cell_type":"code","source":["# Save GNN results\n","gnn_results = {\n","    'val_loss': best_val_loss,\n","    'metrics': metrics,  # Final validation metrics\n","    'train_losses': train_losses,\n","    'val_losses': val_losses,\n","    'val_metrics_history': val_metrics_history\n","}\n","\n","with open(f\"{results_dir}/gnn_results.pkl\", 'wb') as f:\n","    pickle.dump(gnn_results, f)"],"metadata":{"id":"w_TwZX1IaoKX","executionInfo":{"status":"ok","timestamp":1765508579847,"user_tz":480,"elapsed":194,"user":{"displayName":"Vedant Sahu","userId":"07926233457236624514"}}},"execution_count":21,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}